{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. NLP Of Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.0.1-cp36-cp36m-win_amd64.whl (23.9 MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\dev\\lib\\site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: dataclasses in c:\\dev\\lib\\site-packages (from gensim) (0.8)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\dev\\lib\\site-packages (from gensim) (1.2.0)\n",
      "Collecting Cython==0.29.21\n",
      "  Downloading Cython-0.29.21-cp36-cp36m-win_amd64.whl (1.6 MB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\dev\\lib\\site-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied: requests in c:\\dev\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.25.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\dev\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\dev\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\dev\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\dev\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.22)\n",
      "Installing collected packages: Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.28.2\n",
      "    Uninstalling Cython-0.28.2:\n",
      "      Successfully uninstalled Cython-0.28.2\n",
      "Successfully installed Cython-0.29.21 gensim-4.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\dev\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\dev\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\dev\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\dev\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\dev\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\dev\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -tatsmodels (c:\\dev\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -cipy (c:\\dev\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\dev\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\dev\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\dev\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\dev\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\dev\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\dev\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.1.2; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\dev\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Allegra\n",
      "[nltk_data]     Grunberg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Allegra\n",
      "[nltk_data]     Grunberg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import sys\n",
    "#import emoji\n",
    "import spacy\n",
    "import math\n",
    "import pickle\n",
    "from time import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import nltk\n",
    "#import contractions\n",
    "import re\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "wall = pd.read_csv('reddit_wsb.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lem is better then stem dont need to do both\n",
    "#Consider lemmatization (reduce words such as “am”, “are”, and “is” to a common form such as “be”)\n",
    "#spacy.load('en') \n",
    "\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load() # load spacy model, can be \"en_core_web_sm\"\n",
    "\n",
    "# Apply lemmatization in order to get the base form of a word\n",
    "lemmatized = []\n",
    "\n",
    "for text in wall['title']:\n",
    "    doc = nlp(text)\n",
    "    # Lemmatizing each token\n",
    "    mytokens = [word.lemma_ if word.lemma_ != \"-PrON-\" else word.lower_ for word in doc]\n",
    "    join_list = (' ').join(mytokens) # Join list of words into a sentence\n",
    "    lemmatized.append(join_list) # Append to a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove nans\n",
    "\n",
    "wall = wall.dropna(subset=['title', 'body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\dev\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title_split</th>\n",
       "      <th>body_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exit the system</td>\n",
       "      <td>0</td>\n",
       "      <td>l6uhhn</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>47</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>The CEO of NASDAQ pushed to halt trading “to g...</td>\n",
       "      <td>2021-01-28 21:30:35</td>\n",
       "      <td>[Exit, the, system]</td>\n",
       "      <td>[The, CEO, of, NASDAQ, pushed, to, halt, tradi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SHORT STOCK DOESN'T HAVE AN EXPIRATION DATE</td>\n",
       "      <td>317</td>\n",
       "      <td>l6uf6d</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>53</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>Hedgefund whales are spreading disinfo saying ...</td>\n",
       "      <td>2021-01-28 21:26:27</td>\n",
       "      <td>[SHORT, STOCK, DOESN'T, HAVE, AN, EXPIRATION, ...</td>\n",
       "      <td>[Hedgefund, whales, are, spreading, disinfo, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>THIS IS THE MOMENT</td>\n",
       "      <td>405</td>\n",
       "      <td>l6ub9l</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>178</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>Life isn't fair. My mother always told me that...</td>\n",
       "      <td>2021-01-28 21:19:31</td>\n",
       "      <td>[THIS, IS, THE, MOMENT]</td>\n",
       "      <td>[Life, isn't, fair., My, mother, always, told,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>We need to keep this movement going, we all ca...</td>\n",
       "      <td>222</td>\n",
       "      <td>l6uao1</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>70</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>I believe right now is one of those rare oppo...</td>\n",
       "      <td>2021-01-28 21:18:25</td>\n",
       "      <td>[We, need, to, keep, this, movement, going,, w...</td>\n",
       "      <td>[I, believe, right, now, is, one, of, those, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Once you're done with GME - $AG and $SLV, the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>l6u9wu</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>16</td>\n",
       "      <td>1.611861e+09</td>\n",
       "      <td>You guys are champs. GME... who would have tho...</td>\n",
       "      <td>2021-01-28 21:17:10</td>\n",
       "      <td>[Once, you're, done, with, GME, -, $AG, and, $...</td>\n",
       "      <td>[You, guys, are, champs., GME..., who, would, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  score      id  \\\n",
       "2                                     Exit the system      0  l6uhhn   \n",
       "6         SHORT STOCK DOESN'T HAVE AN EXPIRATION DATE    317  l6uf6d   \n",
       "7                                  THIS IS THE MOMENT    405  l6ub9l   \n",
       "10  We need to keep this movement going, we all ca...    222  l6uao1   \n",
       "12  Once you're done with GME - $AG and $SLV, the ...      0  l6u9wu   \n",
       "\n",
       "                                                  url  comms_num  \\\n",
       "2   https://www.reddit.com/r/wallstreetbets/commen...         47   \n",
       "6   https://www.reddit.com/r/wallstreetbets/commen...         53   \n",
       "7   https://www.reddit.com/r/wallstreetbets/commen...        178   \n",
       "10  https://www.reddit.com/r/wallstreetbets/commen...         70   \n",
       "12  https://www.reddit.com/r/wallstreetbets/commen...         16   \n",
       "\n",
       "         created                                               body  \\\n",
       "2   1.611862e+09  The CEO of NASDAQ pushed to halt trading “to g...   \n",
       "6   1.611862e+09  Hedgefund whales are spreading disinfo saying ...   \n",
       "7   1.611862e+09  Life isn't fair. My mother always told me that...   \n",
       "10  1.611862e+09   I believe right now is one of those rare oppo...   \n",
       "12  1.611861e+09  You guys are champs. GME... who would have tho...   \n",
       "\n",
       "              timestamp                                        title_split  \\\n",
       "2   2021-01-28 21:30:35                                [Exit, the, system]   \n",
       "6   2021-01-28 21:26:27  [SHORT, STOCK, DOESN'T, HAVE, AN, EXPIRATION, ...   \n",
       "7   2021-01-28 21:19:31                            [THIS, IS, THE, MOMENT]   \n",
       "10  2021-01-28 21:18:25  [We, need, to, keep, this, movement, going,, w...   \n",
       "12  2021-01-28 21:17:10  [Once, you're, done, with, GME, -, $AG, and, $...   \n",
       "\n",
       "                                           body_split  \n",
       "2   [The, CEO, of, NASDAQ, pushed, to, halt, tradi...  \n",
       "6   [Hedgefund, whales, are, spreading, disinfo, s...  \n",
       "7   [Life, isn't, fair., My, mother, always, told,...  \n",
       "10  [I, believe, right, now, is, one, of, those, r...  \n",
       "12  [You, guys, are, champs., GME..., who, would, ...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize text (split words into a list)\n",
    "def split_line(text):\n",
    "    # split the text\n",
    "    list_words = text.split()\n",
    "    return list_words\n",
    "\n",
    "wall['title_split'] = wall['title'].apply(split_line)\n",
    "wall['body_split'] = wall['body'].apply(split_line)\n",
    "wall.head()  #(change comments to body etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\dev\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title_split</th>\n",
       "      <th>body_split</th>\n",
       "      <th>title_nostop</th>\n",
       "      <th>body_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exit the system</td>\n",
       "      <td>0</td>\n",
       "      <td>l6uhhn</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>47</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>The CEO of NASDAQ pushed to halt trading “to g...</td>\n",
       "      <td>2021-01-28 21:30:35</td>\n",
       "      <td>[Exit, the, system]</td>\n",
       "      <td>[The, CEO, of, NASDAQ, pushed, to, halt, tradi...</td>\n",
       "      <td>[Exit, system]</td>\n",
       "      <td>[The, CEO, NASDAQ, pushed, halt, trading, “to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SHORT STOCK DOESN'T HAVE AN EXPIRATION DATE</td>\n",
       "      <td>317</td>\n",
       "      <td>l6uf6d</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>53</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>Hedgefund whales are spreading disinfo saying ...</td>\n",
       "      <td>2021-01-28 21:26:27</td>\n",
       "      <td>[SHORT, STOCK, DOESN'T, HAVE, AN, EXPIRATION, ...</td>\n",
       "      <td>[Hedgefund, whales, are, spreading, disinfo, s...</td>\n",
       "      <td>[SHORT, STOCK, DOESN'T, HAVE, AN, EXPIRATION, ...</td>\n",
       "      <td>[Hedgefund, whales, spreading, disinfo, saying...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>THIS IS THE MOMENT</td>\n",
       "      <td>405</td>\n",
       "      <td>l6ub9l</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>178</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>Life isn't fair. My mother always told me that...</td>\n",
       "      <td>2021-01-28 21:19:31</td>\n",
       "      <td>[THIS, IS, THE, MOMENT]</td>\n",
       "      <td>[Life, isn't, fair., My, mother, always, told,...</td>\n",
       "      <td>[THIS, IS, THE, MOMENT]</td>\n",
       "      <td>[Life, fair., My, mother, always, told, I, wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>We need to keep this movement going, we all ca...</td>\n",
       "      <td>222</td>\n",
       "      <td>l6uao1</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>70</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>I believe right now is one of those rare oppo...</td>\n",
       "      <td>2021-01-28 21:18:25</td>\n",
       "      <td>[We, need, to, keep, this, movement, going,, w...</td>\n",
       "      <td>[I, believe, right, now, is, one, of, those, r...</td>\n",
       "      <td>[We, need, keep, movement, going,, make, histo...</td>\n",
       "      <td>[I, believe, right, one, rare, opportunities, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Once you're done with GME - $AG and $SLV, the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>l6u9wu</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>16</td>\n",
       "      <td>1.611861e+09</td>\n",
       "      <td>You guys are champs. GME... who would have tho...</td>\n",
       "      <td>2021-01-28 21:17:10</td>\n",
       "      <td>[Once, you're, done, with, GME, -, $AG, and, $...</td>\n",
       "      <td>[You, guys, are, champs., GME..., who, would, ...</td>\n",
       "      <td>[Once, done, GME, -, $AG, $SLV,, gentleman's, ...</td>\n",
       "      <td>[You, guys, champs., GME..., would, thought, b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  score      id  \\\n",
       "2                                     Exit the system      0  l6uhhn   \n",
       "6         SHORT STOCK DOESN'T HAVE AN EXPIRATION DATE    317  l6uf6d   \n",
       "7                                  THIS IS THE MOMENT    405  l6ub9l   \n",
       "10  We need to keep this movement going, we all ca...    222  l6uao1   \n",
       "12  Once you're done with GME - $AG and $SLV, the ...      0  l6u9wu   \n",
       "\n",
       "                                                  url  comms_num  \\\n",
       "2   https://www.reddit.com/r/wallstreetbets/commen...         47   \n",
       "6   https://www.reddit.com/r/wallstreetbets/commen...         53   \n",
       "7   https://www.reddit.com/r/wallstreetbets/commen...        178   \n",
       "10  https://www.reddit.com/r/wallstreetbets/commen...         70   \n",
       "12  https://www.reddit.com/r/wallstreetbets/commen...         16   \n",
       "\n",
       "         created                                               body  \\\n",
       "2   1.611862e+09  The CEO of NASDAQ pushed to halt trading “to g...   \n",
       "6   1.611862e+09  Hedgefund whales are spreading disinfo saying ...   \n",
       "7   1.611862e+09  Life isn't fair. My mother always told me that...   \n",
       "10  1.611862e+09   I believe right now is one of those rare oppo...   \n",
       "12  1.611861e+09  You guys are champs. GME... who would have tho...   \n",
       "\n",
       "              timestamp                                        title_split  \\\n",
       "2   2021-01-28 21:30:35                                [Exit, the, system]   \n",
       "6   2021-01-28 21:26:27  [SHORT, STOCK, DOESN'T, HAVE, AN, EXPIRATION, ...   \n",
       "7   2021-01-28 21:19:31                            [THIS, IS, THE, MOMENT]   \n",
       "10  2021-01-28 21:18:25  [We, need, to, keep, this, movement, going,, w...   \n",
       "12  2021-01-28 21:17:10  [Once, you're, done, with, GME, -, $AG, and, $...   \n",
       "\n",
       "                                           body_split  \\\n",
       "2   [The, CEO, of, NASDAQ, pushed, to, halt, tradi...   \n",
       "6   [Hedgefund, whales, are, spreading, disinfo, s...   \n",
       "7   [Life, isn't, fair., My, mother, always, told,...   \n",
       "10  [I, believe, right, now, is, one, of, those, r...   \n",
       "12  [You, guys, are, champs., GME..., who, would, ...   \n",
       "\n",
       "                                         title_nostop  \\\n",
       "2                                      [Exit, system]   \n",
       "6   [SHORT, STOCK, DOESN'T, HAVE, AN, EXPIRATION, ...   \n",
       "7                             [THIS, IS, THE, MOMENT]   \n",
       "10  [We, need, keep, movement, going,, make, histo...   \n",
       "12  [Once, done, GME, -, $AG, $SLV,, gentleman's, ...   \n",
       "\n",
       "                                          body_nostop  \n",
       "2   [The, CEO, NASDAQ, pushed, halt, trading, “to,...  \n",
       "6   [Hedgefund, whales, spreading, disinfo, saying...  \n",
       "7   [Life, fair., My, mother, always, told, I, wou...  \n",
       "10  [I, believe, right, one, rare, opportunities, ...  \n",
       "12  [You, guys, champs., GME..., would, thought, b...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encountering an error with nltk not being defined in order to download stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Print and copy stopwords instead\n",
    "#print(len(stopwords.words('english')),'\\n', stopwords.words('english'))\n",
    "\n",
    "#//input [‘abc’, ‘bcd’, ‘no’]\n",
    "#//output [‘abc’,’bcd’] \n",
    " \n",
    "def remove_stopwords(my_list):\n",
    "    newlist=[]\n",
    "    for word in my_list:\n",
    "        if word in stopwords.words('english'):\n",
    "            continue\n",
    "        else: \n",
    "            newlist.append(word)\n",
    "    return newlist\n",
    "\n",
    "wall['title_nostop'] = wall['title_split'].apply(remove_stopwords)\n",
    "wall['body_nostop'] = wall['body_split'].apply(remove_stopwords)\n",
    "wall.head()  #(change comments to body etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Requirement already satisfied: requests in c:\\dev\\lib\\site-packages (from vaderSentiment) (2.25.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\dev\\lib\\site-packages (from requests->vaderSentiment) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\dev\\lib\\site-packages (from requests->vaderSentiment) (2.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\dev\\lib\\site-packages (from requests->vaderSentiment) (1.22)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\dev\\lib\\site-packages (from requests->vaderSentiment) (3.0.4)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\dev\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\dev\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\dev\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\dev\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\dev\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\dev\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\dev\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\dev\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\dev\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\dev\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.1.2; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\dev\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/61608057/output-vader-sentiment-scores-in-columns-based-on-dataframe-rows-of-tweets\n",
    "#https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/\n",
    "#https://medium.com/nerd-for-tech/wallstreetbets-sentiment-analysis-on-stock-prices-using-natural-language-processing-ed1e9e109a37\n",
    "#https://swaggystocks.com/dashboard/wallstreetbets/realtime\n",
    "#https://nbviewer.jupyter.org/github/dipanjanS/practical_nlp_workshop_gids20/blob/main/tutorials/03_NLP%20Applications/09_NLP_Applications_Text_Classification_Machine_Learning_and_Basic_DNN.ipynb\n",
    "\n",
    "More \n",
    "#https://github.com/ProsusAI/finBERT \n",
    "#https://huggingface.co/ProsusAI/finbert\n",
    "    \n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\lib\\site-packages\\ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title_split</th>\n",
       "      <th>body_split</th>\n",
       "      <th>title_nostop</th>\n",
       "      <th>body_nostop</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exit the system</td>\n",
       "      <td>0</td>\n",
       "      <td>l6uhhn</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>47</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>The CEO of NASDAQ pushed to halt trading “to g...</td>\n",
       "      <td>2021-01-28 21:30:35</td>\n",
       "      <td>[Exit, the, system]</td>\n",
       "      <td>[The, CEO, of, NASDAQ, pushed, to, halt, tradi...</td>\n",
       "      <td>[Exit, system]</td>\n",
       "      <td>[The, CEO, NASDAQ, pushed, halt, trading, “to,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'neg': 0.07, 'neu': 0.852, 'pos': 0.078, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SHORT STOCK DOESN'T HAVE AN EXPIRATION DATE</td>\n",
       "      <td>317</td>\n",
       "      <td>l6uf6d</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>53</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>Hedgefund whales are spreading disinfo saying ...</td>\n",
       "      <td>2021-01-28 21:26:27</td>\n",
       "      <td>[SHORT, STOCK, DOESN'T, HAVE, AN, EXPIRATION, ...</td>\n",
       "      <td>[Hedgefund, whales, are, spreading, disinfo, s...</td>\n",
       "      <td>[SHORT, STOCK, DOESN'T, HAVE, AN, EXPIRATION, ...</td>\n",
       "      <td>[Hedgefund, whales, spreading, disinfo, saying...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'neg': 0.053, 'neu': 0.835, 'pos': 0.112, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>THIS IS THE MOMENT</td>\n",
       "      <td>405</td>\n",
       "      <td>l6ub9l</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>178</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>Life isn't fair. My mother always told me that...</td>\n",
       "      <td>2021-01-28 21:19:31</td>\n",
       "      <td>[THIS, IS, THE, MOMENT]</td>\n",
       "      <td>[Life, isn't, fair., My, mother, always, told,...</td>\n",
       "      <td>[THIS, IS, THE, MOMENT]</td>\n",
       "      <td>[Life, fair., My, mother, always, told, I, wou...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'neg': 0.177, 'neu': 0.768, 'pos': 0.056, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>We need to keep this movement going, we all ca...</td>\n",
       "      <td>222</td>\n",
       "      <td>l6uao1</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>70</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>I believe right now is one of those rare oppo...</td>\n",
       "      <td>2021-01-28 21:18:25</td>\n",
       "      <td>[We, need, to, keep, this, movement, going,, w...</td>\n",
       "      <td>[I, believe, right, now, is, one, of, those, r...</td>\n",
       "      <td>[We, need, keep, movement, going,, make, histo...</td>\n",
       "      <td>[I, believe, right, one, rare, opportunities, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'neg': 0.093, 'neu': 0.735, 'pos': 0.172, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Once you're done with GME - $AG and $SLV, the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>l6u9wu</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>16</td>\n",
       "      <td>1.611861e+09</td>\n",
       "      <td>You guys are champs. GME... who would have tho...</td>\n",
       "      <td>2021-01-28 21:17:10</td>\n",
       "      <td>[Once, you're, done, with, GME, -, $AG, and, $...</td>\n",
       "      <td>[You, guys, are, champs., GME..., who, would, ...</td>\n",
       "      <td>[Once, done, GME, -, $AG, $SLV,, gentleman's, ...</td>\n",
       "      <td>[You, guys, champs., GME..., would, thought, b...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'neg': 0.068, 'neu': 0.775, 'pos': 0.156, 'co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  score      id  \\\n",
       "2                                     Exit the system      0  l6uhhn   \n",
       "6         SHORT STOCK DOESN'T HAVE AN EXPIRATION DATE    317  l6uf6d   \n",
       "7                                  THIS IS THE MOMENT    405  l6ub9l   \n",
       "10  We need to keep this movement going, we all ca...    222  l6uao1   \n",
       "12  Once you're done with GME - $AG and $SLV, the ...      0  l6u9wu   \n",
       "\n",
       "                                                  url  comms_num  \\\n",
       "2   https://www.reddit.com/r/wallstreetbets/commen...         47   \n",
       "6   https://www.reddit.com/r/wallstreetbets/commen...         53   \n",
       "7   https://www.reddit.com/r/wallstreetbets/commen...        178   \n",
       "10  https://www.reddit.com/r/wallstreetbets/commen...         70   \n",
       "12  https://www.reddit.com/r/wallstreetbets/commen...         16   \n",
       "\n",
       "         created                                               body  \\\n",
       "2   1.611862e+09  The CEO of NASDAQ pushed to halt trading “to g...   \n",
       "6   1.611862e+09  Hedgefund whales are spreading disinfo saying ...   \n",
       "7   1.611862e+09  Life isn't fair. My mother always told me that...   \n",
       "10  1.611862e+09   I believe right now is one of those rare oppo...   \n",
       "12  1.611861e+09  You guys are champs. GME... who would have tho...   \n",
       "\n",
       "              timestamp                                        title_split  \\\n",
       "2   2021-01-28 21:30:35                                [Exit, the, system]   \n",
       "6   2021-01-28 21:26:27  [SHORT, STOCK, DOESN'T, HAVE, AN, EXPIRATION, ...   \n",
       "7   2021-01-28 21:19:31                            [THIS, IS, THE, MOMENT]   \n",
       "10  2021-01-28 21:18:25  [We, need, to, keep, this, movement, going,, w...   \n",
       "12  2021-01-28 21:17:10  [Once, you're, done, with, GME, -, $AG, and, $...   \n",
       "\n",
       "                                           body_split  \\\n",
       "2   [The, CEO, of, NASDAQ, pushed, to, halt, tradi...   \n",
       "6   [Hedgefund, whales, are, spreading, disinfo, s...   \n",
       "7   [Life, isn't, fair., My, mother, always, told,...   \n",
       "10  [I, believe, right, now, is, one, of, those, r...   \n",
       "12  [You, guys, are, champs., GME..., who, would, ...   \n",
       "\n",
       "                                         title_nostop  \\\n",
       "2                                      [Exit, system]   \n",
       "6   [SHORT, STOCK, DOESN'T, HAVE, AN, EXPIRATION, ...   \n",
       "7                             [THIS, IS, THE, MOMENT]   \n",
       "10  [We, need, keep, movement, going,, make, histo...   \n",
       "12  [Once, done, GME, -, $AG, $SLV,, gentleman's, ...   \n",
       "\n",
       "                                          body_nostop  compound  neg  neu  \\\n",
       "2   [The, CEO, NASDAQ, pushed, halt, trading, “to,...       0.0  0.0  1.0   \n",
       "6   [Hedgefund, whales, spreading, disinfo, saying...       0.0  0.0  1.0   \n",
       "7   [Life, fair., My, mother, always, told, I, wou...       0.0  0.0  1.0   \n",
       "10  [I, believe, right, one, rare, opportunities, ...       0.0  0.0  1.0   \n",
       "12  [You, guys, champs., GME..., would, thought, b...       0.0  0.0  1.0   \n",
       "\n",
       "    pos                                             rating  \n",
       "2   0.0  {'neg': 0.07, 'neu': 0.852, 'pos': 0.078, 'com...  \n",
       "6   0.0  {'neg': 0.053, 'neu': 0.835, 'pos': 0.112, 'co...  \n",
       "7   0.0  {'neg': 0.177, 'neu': 0.768, 'pos': 0.056, 'co...  \n",
       "10  0.0  {'neg': 0.093, 'neu': 0.735, 'pos': 0.172, 'co...  \n",
       "12  0.0  {'neg': 0.068, 'neu': 0.775, 'pos': 0.156, 'co...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vader uses normal english may n ot pick up on nuanced languages or context\n",
    "#look at models -- LINKS DJ SENT !! \n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "new_words = {\n",
    "    'citron': -4.0,  \n",
    "    'hidenburg': -4.0,        \n",
    "    'moon': 4.0,\n",
    "    'highs': 2.0,\n",
    "    'mooning': 4.0,\n",
    "    'long': 2.0,\n",
    "    'short': -2.0,\n",
    "    'call': 4.0,\n",
    "    'calls': 4.0,    \n",
    "    'put': -4.0,\n",
    "    'puts': -4.0,    \n",
    "    'break': 2.0,\n",
    "    'tendie': 2.0,\n",
    "     'tendies': 2.0,\n",
    "     'town': 2.0,     \n",
    "     'overvalued': -3.0,\n",
    "     'undervalued': 3.0,\n",
    "     'buy': 4.0,\n",
    "     'sell': -4.0,\n",
    "     'gone': -1.0,\n",
    "     'gtfo': -1.7,\n",
    "     'paper': -1.7,\n",
    "     'bullish': 3.7,\n",
    "     'bearish': -3}\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "analyzer.lexicon.update(new_words)\n",
    "\n",
    "wall['rating'] = wall['body'].apply(analyzer.polarity_scores)\n",
    "\n",
    "#wall['compound'] = [analyzer.polarity_scores(x)['compound'] for x in wall['body_split']]\n",
    "#wall['neg'] = [analyzer.polarity_scores(x)['neg'] for x in wall['body_split']]\n",
    "#wall['neu'] = [analyzer.polarity_scores(x)['neu'] for x in wall['body_split']]\n",
    "#wall['pos'] = [analyzer.polarity_scores(x)['pos'] for x in wall['body_split']]\n",
    "\n",
    "\n",
    "wall.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Allegra\n",
      "[nltk_data]     Grunberg\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-15366275f08a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSIA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mwall\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiments'\u001b[0m\u001b[1;33m]\u001b[0m           \u001b[1;33m=\u001b[0m \u001b[0mwall\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body_nostop'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\w+'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mwall\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Positive Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mwall\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiments'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pos'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mwall\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Neutral Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mwall\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiments'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'neu'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4211\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4212\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4213\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4215\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-84-15366275f08a>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSIA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mwall\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiments'\u001b[0m\u001b[1;33m]\u001b[0m           \u001b[1;33m=\u001b[0m \u001b[0mwall\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body_nostop'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\w+'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mwall\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Positive Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mwall\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiments'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pos'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mwall\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Neutral Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mwall\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiments'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'neu'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "#sentiment analysis take 2 \n",
    "#try this with body\n",
    "#same as vader! \n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "sid = SIA()\n",
    "wall['sentiments']           = wall['body_nostop'].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\n",
    "wall['Positive Sentiment']   = wall['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \n",
    "wall['Neutral Sentiment']    = wall['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\n",
    "wall['Negative Sentiment']   = wall['sentiments'].apply(lambda x: x['neg']+1*(10**-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Converting Emoji's to meanings\n",
    "\n",
    "- https://www.kaggle.com/iamprateek/emoji-analysis\n",
    "- https://www.geeksforgeeks.org/convert-emoji-into-text-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['Pos', 'Neut', 'Neg', 'Char']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-0f1b43d52d93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0muse_cols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Char'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Neg'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Neut'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Pos'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'reddit_wsb.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#wall = pd.read_csv('reddit_wsb.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2054\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morig_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2055\u001b[0m             ):\n\u001b[1;32m-> 2056\u001b[1;33m                 \u001b[0m_validate_usecols_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morig_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2057\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2058\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_validate_usecols_names\u001b[1;34m(usecols, names)\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         raise ValueError(\n\u001b[1;32m-> 1305\u001b[1;33m             \u001b[1;34mf\"Usecols do not match columns, columns expected but not found: {missing}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1306\u001b[0m         )\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['Pos', 'Neut', 'Neg', 'Char']"
     ]
    }
   ],
   "source": [
    "use_cols=['Char','Neg','Neut','Pos']\n",
    "train_df=pd.read_csv('reddit_wsb.csv',usecols=use_cols)\n",
    "train_df.head(20)\n",
    "\n",
    "#wall = pd.read_csv('reddit_wsb.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the first row\n",
    "train_df.drop(train_df.index[0], inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the index\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change into pandas category\n",
    "train_df[\"Char\"] = train_df[\"Char\"].astype('category')\n",
    "train_df[\"Neg\"] = train_df[\"Neg\"].astype('float')\n",
    "train_df[\"Neut\"] = train_df[\"Neut\"].astype('float')\n",
    "train_df[\"Pos\"] = train_df[\"Pos\"].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Char_cat\"] = train_df[\"Char\"].cat.codes\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=train_df.drop(['Char'],axis=1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values='NaN', strategy='median', axis=0)\n",
    "arrays = dataset.values\n",
    "imp.fit(arrays)\n",
    "array_imp = imp.transform(arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_imp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=array_imp[:,-1]\n",
    "print(X.shape)\n",
    "X=X.reshape(-1,1)\n",
    "Y=array_imp[:,3]\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 0.20\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modeling \n",
    "\n",
    "from sklearn import model_selection\n",
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LogisticRegression()\n",
    "svc.fit(X_train, Y_train)\n",
    "predictions = svc.predict(X_validation)\n",
    "print(accuracy_score(Y_validation, predictions))\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print(classification_report(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's test\n",
    "x = ['👍']\n",
    "arr=np.array(x)\n",
    "x_encoded=arr.reshape(-1,1)\n",
    "#x_encoded = x.cat.codes\n",
    "svc.predict(x_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wall_NLP.to_csv('output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
