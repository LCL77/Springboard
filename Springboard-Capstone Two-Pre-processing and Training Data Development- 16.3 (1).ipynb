{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD IN TOTAL HAPPINESS DATA and edu data  ONCE EVERYTHING IS FIXED IN WRAINGLING PART "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Out of all the variables over the 5 years which features most strongly attributed to a happier country.\n",
    "\n",
    "2. Broken down by regions of the world over the past 5 years- which features strongly attributed to a happier country- is it different per region?\n",
    "\n",
    "3. Take happiest 3 countries and least happy 3 countires in the past 4 years and look up datasets for, is there any correlation:\n",
    "\n",
    "- money spent on education (% of GDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why can't I drop corresponding categorical columns and first dummy variable columns.\n",
    "2. Standardize the magnitude of numeric features using a scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources \n",
    "\n",
    "1.https://ddf46429.springboard.com/uploads/resources/1584729764_Capstone_Two_Step_4__Preprocessing_Training_Data_Developmen_fIzkPuL.pdf\n",
    "2.https://aiden-dataminer.medium.com/the-data-science-method-dsm-pre-processing-and-training-data-development-fd2d75182967\n",
    "3.https://medium.com/@urvashilluniya/convert-multiple-categorical-columns-into-numeric-columns-in-single-line-of-code-577bab825635\n",
    "4.https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/\n",
    "5.https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directions\n",
    "\n",
    "The goal of the preprocessing work is to prepare your data\n",
    "for fitting models. If you identified some categorical features in your dataset in the EDA\n",
    "step, now is the time to create dummy features to allow for the inclusion of those\n",
    "features in your model development. Additionally, standardizing your features numeric\n",
    "magnitude and creating train and test splits happen in this step. You may want to save a\n",
    "version of your clean, preprocessed data frame as a CSV to access later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np \n",
    "import json\n",
    "import statistics\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.datasets import load_iris \n",
    "\n",
    "\n",
    "data_2020= pd.read_csv('2020.csv')\n",
    "\n",
    "data_2019= pd.read_csv('2019.csv')\n",
    "\n",
    "data_2018= pd.read_csv('2018.csv')\n",
    "\n",
    "data_2017= pd.read_csv('2017.csv')\n",
    "\n",
    "data_2016= pd.read_csv('2016.csv')\n",
    "\n",
    "data_2015= pd.read_csv('2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the column \"Year\" to all datasets\n",
    "\n",
    "data_2015['Year'] = 2015 \n",
    "data_2016['Year'] = 2016 \n",
    "data_2017['Year'] = 2017 \n",
    "data_2018['Year'] = 2018    \n",
    "data_2019['Year'] = 2019 \n",
    "data_2020['Year'] = 2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change columns, Country, Region, Happiness Rank, Happiness Score, GDP, Life Expectancy to be the same in all datasets\n",
    "#https://www.listendata.com/2020/09/How-to-rename-columns-in-Pandas.html\n",
    "#call rename () method \n",
    "##df.rename(columns={'year':'years', 'month':'months' }, inplace = True)\n",
    "\n",
    "data_2020.rename(columns={'Social support':'Family','Freedom to make life choices':'Freedom','Perceptions of corruption':'Trust','Country name':'Country','Regional indicator':'Region','Ladder score':'Happiness Score','Logged GDP per capita':'GDP', 'Healthy life expectancy':'Life Expectancy'}, inplace = True)\n",
    "data_2019.rename(columns={'Social support':'Family','Freedom to make life choices':'Freedom','Perceptions of corruption':'Trust','Country or region':'Country','Overall rank':'Happiness Rank','Score':'Happiness Score','GDP per capita':'GDP','Healthy life expectancy':'Life Expectancy'}, inplace = True)\n",
    "data_2018.rename(columns={'Social support':'Family','Freedom to make life choices':'Freedom','Perceptions of corruption':'Trust','Country or region':'Country','Overall rank':'Happiness Rank','Score':'Happiness Score','GDP per capita':'GDP','Healthy life expectancy':'Life Expectancy'}, inplace = True)\n",
    "data_2017.rename(columns={'Trust..Government.Corruption.':'Trust','Country':'Country','Region':'Region','Happiness.Rank':'Happiness Rank','Happiness.Score':'Happiness Score','Economy..GDP.per.Capita.':'GDP','Health..Life.Expectancy.':'Life Expectancy'}, inplace = True)\n",
    "data_2016.rename(columns={'Trust (Government Corruption)':'Trust','Country':'Country','Region':'Region','Happiness Rank':'Happiness Rank','Happiness score':'Happiness Score','Economy (GDP per Capita)':'GDP','Health (Life Expectancy)':'Life Expectancy'}, inplace = True)\n",
    "data_2015.rename(columns={'Trust (Government Corruption)':'Trust','Country':'Country','Region':'Region','Happiness Rank':'Happiness Rank','Happiness score':'Happiness Score','Economy (GDP per Capita)':'GDP','Health (Life Expectancy)':'Life Expectancy'}, inplace = True)\n",
    "#2020 doesnt have a \"happiness rank column\"\n",
    "#2018 doesnt have a region column same as country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate data\n",
    "#resources- https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html \n",
    "    \n",
    "frames = [data_2020, data_2019, data_2018, data_2017, data_2016, data_2015]\n",
    "all_happiness_data = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 935 entries, 0 to 157\n",
      "Data columns (total 29 columns):\n",
      " #   Column                                      Non-Null Count  Dtype  \n",
      "---  ------                                      --------------  -----  \n",
      " 0   Country                                     935 non-null    object \n",
      " 1   Region                                      468 non-null    object \n",
      " 2   Happiness Score                             935 non-null    float64\n",
      " 3   Standard error of ladder score              153 non-null    float64\n",
      " 4   upperwhisker                                153 non-null    float64\n",
      " 5   lowerwhisker                                153 non-null    float64\n",
      " 6   GDP                                         935 non-null    float64\n",
      " 7   Family                                      935 non-null    float64\n",
      " 8   Life Expectancy                             935 non-null    float64\n",
      " 9   Freedom                                     935 non-null    float64\n",
      " 10  Generosity                                  935 non-null    float64\n",
      " 11  Trust                                       934 non-null    float64\n",
      " 12  Ladder score in Dystopia                    153 non-null    float64\n",
      " 13  Explained by: Log GDP per capita            153 non-null    float64\n",
      " 14  Explained by: Social support                153 non-null    float64\n",
      " 15  Explained by: Healthy life expectancy       153 non-null    float64\n",
      " 16  Explained by: Freedom to make life choices  153 non-null    float64\n",
      " 17  Explained by: Generosity                    153 non-null    float64\n",
      " 18  Explained by: Perceptions of corruption     153 non-null    float64\n",
      " 19  Dystopia + residual                         153 non-null    float64\n",
      " 20  Year                                        935 non-null    int64  \n",
      " 21  Happiness Rank                              782 non-null    float64\n",
      " 22  Whisker.high                                155 non-null    float64\n",
      " 23  Whisker.low                                 155 non-null    float64\n",
      " 24  Dystopia.Residual                           155 non-null    float64\n",
      " 25  Lower Confidence Interval                   157 non-null    float64\n",
      " 26  Upper Confidence Interval                   157 non-null    float64\n",
      " 27  Dystopia Residual                           315 non-null    float64\n",
      " 28  Standard Error                              158 non-null    float64\n",
      "dtypes: float64(26), int64(1), object(2)\n",
      "memory usage: 219.1+ KB\n"
     ]
    }
   ],
   "source": [
    "#new dataset info\n",
    "all_happiness_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Training Data Development\n",
    "\n",
    "Goal: Create a cleaned development dataset you can use to complete the\n",
    "modeling step of your project. Pre-processing is the concept of standardizing your model development dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dummy or indicator features for categorical variables\n",
    "- Hint: you’ll need to think about your old favorite pandas functions here like get_dummies() . Consult this guide for help.\n",
    "- First, we select all the columns that are categorical which are those with the data type = ‘object’, creating a data frame subset named ‘dfo’. Next, we concatenate the original data frame df while dropping those columns selected in the dfo, df.drop(dfo,axis=1), with the pandas.get_dummies(dfo) command, creating only indicator columns for the selected object data type columns and collating it with other numeric data frame columns.\n",
    "- you do not pass your dummy aka indicator features to the scaler; they do not need to be scaled as they as are boolean representations of categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select all the columns that are categorical, ‘object’, creating a data frame subset \n",
    "dummy = pd.get_dummies(all_happiness_data['Country'])\n",
    "dummy2 = pd.get_dummies(all_happiness_data['Region'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Afghanistan</th>\n",
       "      <th>Albania</th>\n",
       "      <th>Algeria</th>\n",
       "      <th>Angola</th>\n",
       "      <th>Argentina</th>\n",
       "      <th>Armenia</th>\n",
       "      <th>Australia</th>\n",
       "      <th>Austria</th>\n",
       "      <th>Azerbaijan</th>\n",
       "      <th>Bahrain</th>\n",
       "      <th>...</th>\n",
       "      <th>United Arab Emirates</th>\n",
       "      <th>United Kingdom</th>\n",
       "      <th>United States</th>\n",
       "      <th>Uruguay</th>\n",
       "      <th>Uzbekistan</th>\n",
       "      <th>Venezuela</th>\n",
       "      <th>Vietnam</th>\n",
       "      <th>Yemen</th>\n",
       "      <th>Zambia</th>\n",
       "      <th>Zimbabwe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 172 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Afghanistan  Albania  Algeria  Angola  Argentina  Armenia  Australia  \\\n",
       "0            0        0        0       0          0        0          0   \n",
       "1            0        0        0       0          0        0          0   \n",
       "2            0        0        0       0          0        0          0   \n",
       "3            0        0        0       0          0        0          0   \n",
       "4            0        0        0       0          0        0          0   \n",
       "\n",
       "   Austria  Azerbaijan  Bahrain  ...  United Arab Emirates  United Kingdom  \\\n",
       "0        0           0        0  ...                     0               0   \n",
       "1        0           0        0  ...                     0               0   \n",
       "2        0           0        0  ...                     0               0   \n",
       "3        0           0        0  ...                     0               0   \n",
       "4        0           0        0  ...                     0               0   \n",
       "\n",
       "   United States  Uruguay  Uzbekistan  Venezuela  Vietnam  Yemen  Zambia  \\\n",
       "0              0        0           0          0        0      0       0   \n",
       "1              0        0           0          0        0      0       0   \n",
       "2              0        0           0          0        0      0       0   \n",
       "3              0        0           0          0        0      0       0   \n",
       "4              0        0           0          0        0      0       0   \n",
       "\n",
       "   Zimbabwe  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "\n",
       "[5 rows x 172 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australia and New Zealand</th>\n",
       "      <th>Central and Eastern Europe</th>\n",
       "      <th>Commonwealth of Independent States</th>\n",
       "      <th>East Asia</th>\n",
       "      <th>Eastern Asia</th>\n",
       "      <th>Latin America and Caribbean</th>\n",
       "      <th>Middle East and North Africa</th>\n",
       "      <th>Middle East and Northern Africa</th>\n",
       "      <th>North America</th>\n",
       "      <th>North America and ANZ</th>\n",
       "      <th>South Asia</th>\n",
       "      <th>Southeast Asia</th>\n",
       "      <th>Southeastern Asia</th>\n",
       "      <th>Southern Asia</th>\n",
       "      <th>Sub-Saharan Africa</th>\n",
       "      <th>Western Europe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Australia and New Zealand  Central and Eastern Europe  \\\n",
       "0                          0                           0   \n",
       "1                          0                           0   \n",
       "2                          0                           0   \n",
       "3                          0                           0   \n",
       "4                          0                           0   \n",
       "\n",
       "   Commonwealth of Independent States  East Asia  Eastern Asia  \\\n",
       "0                                   0          0             0   \n",
       "1                                   0          0             0   \n",
       "2                                   0          0             0   \n",
       "3                                   0          0             0   \n",
       "4                                   0          0             0   \n",
       "\n",
       "   Latin America and Caribbean  Middle East and North Africa  \\\n",
       "0                            0                             0   \n",
       "1                            0                             0   \n",
       "2                            0                             0   \n",
       "3                            0                             0   \n",
       "4                            0                             0   \n",
       "\n",
       "   Middle East and Northern Africa  North America  North America and ANZ  \\\n",
       "0                                0              0                      0   \n",
       "1                                0              0                      0   \n",
       "2                                0              0                      0   \n",
       "3                                0              0                      0   \n",
       "4                                0              0                      0   \n",
       "\n",
       "   South Asia  Southeast Asia  Southeastern Asia  Southern Asia  \\\n",
       "0           0               0                  0              0   \n",
       "1           0               0                  0              0   \n",
       "2           0               0                  0              0   \n",
       "3           0               0                  0              0   \n",
       "4           0               0                  0              0   \n",
       "\n",
       "   Sub-Saharan Africa  Western Europe  \n",
       "0                   0               1  \n",
       "1                   0               1  \n",
       "2                   0               1  \n",
       "3                   0               1  \n",
       "4                   0               1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Country          Region  Happiness Score  \\\n",
      "0      Finland  Western Europe           7.8087   \n",
      "1      Denmark  Western Europe           7.6456   \n",
      "2  Switzerland  Western Europe           7.5599   \n",
      "3      Iceland  Western Europe           7.5045   \n",
      "4       Norway  Western Europe           7.4880   \n",
      "\n",
      "   Standard error of ladder score  upperwhisker  lowerwhisker        GDP  \\\n",
      "0                        0.031156      7.869766      7.747634  10.639267   \n",
      "1                        0.033492      7.711245      7.579955  10.774001   \n",
      "2                        0.035014      7.628528      7.491272  10.979933   \n",
      "3                        0.059616      7.621347      7.387653  10.772559   \n",
      "4                        0.034837      7.556281      7.419719  11.087804   \n",
      "\n",
      "     Family  Life Expectancy   Freedom  ...  United Arab Emirates  \\\n",
      "0  0.954330        71.900825  0.949172  ...                     0   \n",
      "1  0.955991        72.402504  0.951444  ...                     0   \n",
      "2  0.942847        74.102448  0.921337  ...                     0   \n",
      "3  0.974670        73.000000  0.948892  ...                     0   \n",
      "4  0.952487        73.200783  0.955750  ...                     0   \n",
      "\n",
      "   United Kingdom  United States  Uruguay  Uzbekistan  Venezuela  Vietnam  \\\n",
      "0               0              0        0           0          0        0   \n",
      "1               0              0        0           0          0        0   \n",
      "2               0              0        0           0          0        0   \n",
      "3               0              0        0           0          0        0   \n",
      "4               0              0        0           0          0        0   \n",
      "\n",
      "   Yemen  Zambia  Zimbabwe  \n",
      "0      0       0         0  \n",
      "1      0       0         0  \n",
      "2      0       0         0  \n",
      "3      0       0         0  \n",
      "4      0       0         0  \n",
      "\n",
      "[5 rows x 217 columns]\n"
     ]
    }
   ],
   "source": [
    "#concatenate the original data frame df while dropping those columns selected\n",
    "\n",
    "#Concat new columns to original dataframe \n",
    "df_concat = pd.concat([all_happiness_data, dummy2, dummy], axis=1)\n",
    "print (df_concat.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Country' 'Region'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-40166d638e48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Drop corresponding categorical columns and first dummy variable columns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_concat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Country'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Region'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_concat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4172\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4173\u001b[0m             \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4174\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4175\u001b[0m         )\n\u001b[0;32m   4176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3887\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3888\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3889\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3891\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3921\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3922\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3923\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3924\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3925\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5285\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5286\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5287\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5288\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5289\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Country' 'Region'] not found in axis\""
     ]
    }
   ],
   "source": [
    "#Drop corresponding categorical columns and first dummy variable columns.\n",
    "\n",
    "df_concat.drop(['Country','Region'], inplace=True, axis=1)\n",
    "print(df_concat.head())\n",
    "\n",
    "df.drop(dfo,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into testing and training datasets \n",
    "- Hint: don’t forget your sklearn functions here, like train_test_split(). \n",
    "- we apply it in two steps, first defining the scaler based on the mean and standard deviation of the training data and then applying that scaler to each the training and testing sets.\n",
    "- Implementing a data subset approach with a train and test split with a 70/30 or 80/20 split is the general rule for an effective holdout test data for model validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#train_test_split()\n",
    "# spliting training and testing data\n",
    "#x= data, y=target dependent variable\n",
    "\n",
    "X = all_happiness_data\n",
    "y = all_happiness_data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of original dataset : (935, 29)\n",
      "shape of input - training set (748, 29)\n",
      "shape of output - training set (748, 29)\n",
      "shape of input - testing set (187, 29)\n",
      "shape of output - testing set (187, 29)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of original dataset :\", all_happiness_data.shape)\n",
    "print(\"shape of input - training set\", X_train.shape)\n",
    "print(\"shape of output - training set\", y_train.shape)\n",
    "print(\"shape of input - testing set\", X_test.shape)\n",
    "print(\"shape of output - testing set\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#train_test_split()\n",
    "# spliting training and testing data\n",
    "#x= data, y=target dependent variable\n",
    "\n",
    "X = all_happiness_data\n",
    "y = all_happiness_data.GDP\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of original dataset : (935, 29)\n",
      "shape of input - training set (748, 29)\n",
      "shape of output - training set (748,)\n",
      "shape of input - testing set (187, 29)\n",
      "shape of output - testing set (187,)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of original dataset :\", all_happiness_data.shape)\n",
    "print(\"shape of input - training set\", X_train.shape)\n",
    "print(\"shape of output - training set\", y_train.shape)\n",
    "print(\"shape of input - testing set\", X_test.shape)\n",
    "print(\"shape of output - testing set\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happiness Score                                  5.394447\n",
      "Standard error of ladder score                   0.053539\n",
      "upperwhisker                                     5.663600\n",
      "lowerwhisker                                     5.453727\n",
      "GDP                                              2.284421\n",
      "Family                                           1.034573\n",
      "Life Expectancy                                 11.063895\n",
      "Freedom                                          0.470323\n",
      "Generosity                                       0.180960\n",
      "Trust                                            0.224161\n",
      "Ladder score in Dystopia                         1.972317\n",
      "Explained by: Log GDP per capita                 0.873163\n",
      "Explained by: Social support                     1.169271\n",
      "Explained by: Healthy life expectancy            0.701517\n",
      "Explained by: Freedom to make life choices       0.470551\n",
      "Explained by: Generosity                         0.193262\n",
      "Explained by: Perceptions of corruption          0.131397\n",
      "Dystopia + residual                              2.019503\n",
      "Year                                          2017.447861\n",
      "Happiness Rank                                  79.436102\n",
      "Whisker.high                                     5.390238\n",
      "Whisker.low                                      5.192275\n",
      "Dystopia.Residual                                1.814012\n",
      "Lower Confidence Interval                        5.315659\n",
      "Upper Confidence Interval                        5.515411\n",
      "Dystopia Residual                                2.219120\n",
      "Standard Error                                   0.048058\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#what is the mean of training data\n",
    "\n",
    "print(X_train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.284420544464335\n"
     ]
    }
   ],
   "source": [
    "print(y_train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happiness Score                               1.115786e+00\n",
      "Standard error of ladder score                1.959381e-02\n",
      "upperwhisker                                  1.126050e+00\n",
      "lowerwhisker                                  1.160814e+00\n",
      "GDP                                           3.165449e+00\n",
      "Family                                        3.116965e-01\n",
      "Life Expectancy                               2.385765e+01\n",
      "Freedom                                       2.038308e-01\n",
      "Generosity                                    1.513460e-01\n",
      "Trust                                         2.557612e-01\n",
      "Ladder score in Dystopia                      2.675523e-15\n",
      "Explained by: Log GDP per capita              3.839834e-01\n",
      "Explained by: Social support                  2.965560e-01\n",
      "Explained by: Healthy life expectancy         2.534086e-01\n",
      "Explained by: Freedom to make life choices    1.412818e-01\n",
      "Explained by: Generosity                      9.946854e-02\n",
      "Explained by: Perceptions of corruption       1.157491e-01\n",
      "Dystopia + residual                           4.976373e-01\n",
      "Year                                          1.721961e+00\n",
      "Happiness Rank                                4.442417e+01\n",
      "Whisker.high                                  1.120777e+00\n",
      "Whisker.low                                   1.144050e+00\n",
      "Dystopia.Residual                             4.793611e-01\n",
      "Lower Confidence Interval                     1.114676e+00\n",
      "Upper Confidence Interval                     1.106210e+00\n",
      "Dystopia Residual                             5.743058e-01\n",
      "Standard Error                                1.746544e-02\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#what is the STD of the training data\n",
    "\n",
    "print(X_train.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.165449337814116\n"
     ]
    }
   ],
   "source": [
    "print(y_train.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize the magnitude of numeric features using a scale\n",
    "\n",
    "- Hint: you might need to employ Python code like this: Making a Scaler object\n",
    "\n",
    "- scaler = preprocessing.StandardScaler()\n",
    "\n",
    "Fitting data to the scaler object\n",
    "\n",
    "- scaled_df = scaler.fit_transform(df)\n",
    "- scaled_df = pd.DataFrame(scaled_df, columns=names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Finland'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-73d03616e4b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# fit and transform the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mscaled_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_happiness_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    610\u001b[0m         \"\"\"\n\u001b[0;32m    611\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[1;32m--> 612\u001b[1;33m                         warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[1;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    431\u001b[0m                                       force_all_finite)\n\u001b[0;32m    432\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1780\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1781\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1783\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Finland'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# the scaler object (model)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit and transform the data\n",
    "scaled_data = scaler.fit_transform(all_happiness_data) \n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaled_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-e329bb43fbdc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Verify that the mean of each feature (column) is 0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mscaled_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scaled_data' is not defined"
     ]
    }
   ],
   "source": [
    "#Verify that the mean of each feature (column) is 0:\n",
    "\n",
    "scaled_data.mean(axis = 0)\n",
    "array([0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify that the std of each feature (column) is 1:\n",
    "\n",
    "scaled_data.std(axis = 0)\n",
    "array([1., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratifying Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-96783d154d0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m X_train, X_test, y_train, y_test = train_test_split(X, y,\n\u001b[0;32m      5\u001b[0m                                                     \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_happiness_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'GDP'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                                                     test_size=0.25)\n\u001b[0m",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2054\u001b[0m                      random_state=random_state)\n\u001b[0;32m   2055\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2056\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2057\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2058\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1202\u001b[0m         \"\"\"\n\u001b[0;32m   1203\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1204\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1205\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_iter_indices\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1544\u001b[0m         \u001b[0mclass_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1545\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_counts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1546\u001b[1;33m             raise ValueError(\"The least populated class in y has only 1\"\n\u001b[0m\u001b[0;32m   1547\u001b[0m                              \u001b[1;34m\" member, which is too few. The minimum\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1548\u001b[0m                              \u001b[1;34m\" number of groups for any class cannot\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "#Does the data need to be stratified\n",
    "#https://datascience.stackexchange.com/questions/40584/meaning-of-stratify-parameter#:~:text=The%20stratify%20parameter%20asks%20whether,stratify%20%3D%20True%20%2C%20with%20a%20.\n",
    "#No because there's only one class for GDP\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    stratify=all_happiness_data['GDP'], \n",
    "                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review the following questions and apply them to your dataset:\n",
    "\n",
    "- Does my data set have any categorical data, such as Gender or day of the week?\n",
    "\n",
    "Yes, Country and Region. \n",
    "\n",
    "- Do my features have data values that range from 0 - 100 or 0-1 or both and more? \n",
    "\n",
    "Yes, both. Features like GDP, Healthy life expectancy are 0-100, Ladder Score is 0-10, and Social Support, Genorosity and the majority of the other features are on a 0-1 scale. \n",
    "- https://en.wikipedia.org/wiki/World_Happiness_Report#Methodology\n",
    "- https://worldhappiness.report/faq/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input array\n",
    "X_predict = targets[['petal length (cm)', 'petal width (cm)']]\n",
    "\n",
    "\n",
    "# Predict with the model\n",
    "predictions = model.predict(X_predict)\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "# Visualize predictions and actual values\n",
    "plt.scatter(X_predict['petal length (cm)'], X_predict['petal width (cm)'],\n",
    "            c=predictions, cmap=plt.cm.coolwarm)\n",
    "plt.title(\"Predicted class values\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy indicator \n",
    "\n",
    "get_dummies()\n",
    "\n",
    "dfo=df.select_dtypes(include=['object']) # select object type columns\n",
    "df = pd.concat([df.drop(dfo, axis=1), pd.get_dummies(dfo)], axis=1)\n",
    "\n",
    "# data standardization with  sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# copy of datasets\n",
    "X_train_stand = X_train.copy()\n",
    "X_test_stand = X_test.copy()\n",
    "\n",
    "# numerical features\n",
    "num_cols = ['Item_Weight','Item_Visibility','Item_MRP','Outlet_Establishment_Year']\n",
    "\n",
    "# apply standardization on numerical features\n",
    "for i in num_cols:\n",
    "    \n",
    "    # fit on training data column\n",
    "    scale = StandardScaler().fit(X_train_stand[[i]])\n",
    "    \n",
    "    # transform the training data column\n",
    "    X_train_stand[i] = scale.transform(X_train_stand[[i]])\n",
    "    \n",
    "    # transform the testing data column\n",
    "    X_test_stand[i] = scale.transform(X_test_stand[[i]])\n",
    "    \n",
    "\n",
    "scaler = StandardScaler()\n",
    "df2 = data_2020.DataFrame(scaler.fit_transform(df),\n",
    "                   columns=['WEIGHT','PRICE'],\n",
    "                   index = ['Orange','Apple','Banana','Grape'])\n",
    "ax = df.plot.scatter(x='WEIGHT', y='PRICE',color=['red','green','blue','yellow'], \n",
    "                     marker = '*',s=80, label='BREFORE SCALING');\n",
    "df2.plot.scatter(x='WEIGHT', y='PRICE', color=['red','green','blue','yellow'],\n",
    "                 marker = 'o',s=60,label='AFTER SCALING', ax = ax)\n",
    "plt.axhline(0, color='red',alpha=0.2)\n",
    "plt.axvline(0, color='red',alpha=0.2);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
