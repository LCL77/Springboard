{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\dev\\lib\\site-packages (3.3)\n",
      "Requirement already satisfied: six in c:\\dev\\lib\\site-packages (from nltk) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib as plt\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import datetime\n",
    "!pip install nltk\n",
    "\n",
    "wall = pd.read_csv('reddit_wsb.csv')\n",
    "#https://www.kaggle.com/gpreda/reddit-wallstreetsbets-posts \n",
    "#https://www.kaggle.com/gpreda/wallstreetbets-reddit-posts-analysis\n",
    "#https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk\n",
    "#https://datavizcatalogue.com/search/time.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7c1a7eb1c0ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentiment\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Narrative\n",
    "\n",
    "I was hired by a hedge fund to analyze Reddit post data, to try to pinpoint how and when new trends arise (which comments or users initiate actual movement on the stock market and actual trades. I'm trying to find the most popular topics, and words in the body of the post. I'm also trying to find the correlation between time and day and when a popular comments are posted. \n",
    "\n",
    "By analyzing the data can the hedge fund, get ahead of trends on reddit and thus maybe the markets. I'm looking to pinpoint the signal in the noise (who, when, and what)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's not about the money, it's about sending a...</td>\n",
       "      <td>55</td>\n",
       "      <td>l6ulcx</td>\n",
       "      <td>https://v.redd.it/6j75regs72e61</td>\n",
       "      <td>6</td>\n",
       "      <td>1.611863e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:37:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Math Professor Scott Steiner says the numbers ...</td>\n",
       "      <td>110</td>\n",
       "      <td>l6uibd</td>\n",
       "      <td>https://v.redd.it/ah50lyny62e61</td>\n",
       "      <td>23</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:32:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exit the system</td>\n",
       "      <td>0</td>\n",
       "      <td>l6uhhn</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>47</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>The CEO of NASDAQ pushed to halt trading “to g...</td>\n",
       "      <td>2021-01-28 21:30:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEW SEC FILING FOR GME! CAN SOMEONE LESS RETAR...</td>\n",
       "      <td>29</td>\n",
       "      <td>l6ugk6</td>\n",
       "      <td>https://sec.report/Document/0001193125-21-019848/</td>\n",
       "      <td>74</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:28:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not to distract from GME, just thought our AMC...</td>\n",
       "      <td>71</td>\n",
       "      <td>l6ufgy</td>\n",
       "      <td>https://i.redd.it/4h2sukb662e61.jpg</td>\n",
       "      <td>156</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:26:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score      id  \\\n",
       "0  It's not about the money, it's about sending a...     55  l6ulcx   \n",
       "1  Math Professor Scott Steiner says the numbers ...    110  l6uibd   \n",
       "2                                    Exit the system      0  l6uhhn   \n",
       "3  NEW SEC FILING FOR GME! CAN SOMEONE LESS RETAR...     29  l6ugk6   \n",
       "4  Not to distract from GME, just thought our AMC...     71  l6ufgy   \n",
       "\n",
       "                                                 url  comms_num       created  \\\n",
       "0                    https://v.redd.it/6j75regs72e61          6  1.611863e+09   \n",
       "1                    https://v.redd.it/ah50lyny62e61         23  1.611862e+09   \n",
       "2  https://www.reddit.com/r/wallstreetbets/commen...         47  1.611862e+09   \n",
       "3  https://sec.report/Document/0001193125-21-019848/         74  1.611862e+09   \n",
       "4                https://i.redd.it/4h2sukb662e61.jpg        156  1.611862e+09   \n",
       "\n",
       "                                                body            timestamp  \n",
       "0                                                NaN  2021-01-28 21:37:41  \n",
       "1                                                NaN  2021-01-28 21:32:10  \n",
       "2  The CEO of NASDAQ pushed to halt trading “to g...  2021-01-28 21:30:35  \n",
       "3                                                NaN  2021-01-28 21:28:57  \n",
       "4                                                NaN  2021-01-28 21:26:56  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the head \n",
    "wall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37155 entries, 0 to 37154\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   title      37155 non-null  object \n",
      " 1   score      37155 non-null  int64  \n",
      " 2   id         37155 non-null  object \n",
      " 3   url        37155 non-null  object \n",
      " 4   comms_num  37155 non-null  int64  \n",
      " 5   created    37155 non-null  float64\n",
      " 6   body       18718 non-null  object \n",
      " 7   timestamp  37155 non-null  object \n",
      "dtypes: float64(1), int64(2), object(5)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "#check info \n",
    "wall.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's not about the money, it's about sending a...</td>\n",
       "      <td>l6ulcx</td>\n",
       "      <td>https://v.redd.it/6j75regs72e61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:37:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Math Professor Scott Steiner says the numbers ...</td>\n",
       "      <td>l6uibd</td>\n",
       "      <td>https://v.redd.it/ah50lyny62e61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:32:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exit the system</td>\n",
       "      <td>l6uhhn</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>The CEO of NASDAQ pushed to halt trading “to g...</td>\n",
       "      <td>2021-01-28 21:30:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEW SEC FILING FOR GME! CAN SOMEONE LESS RETAR...</td>\n",
       "      <td>l6ugk6</td>\n",
       "      <td>https://sec.report/Document/0001193125-21-019848/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:28:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not to distract from GME, just thought our AMC...</td>\n",
       "      <td>l6ufgy</td>\n",
       "      <td>https://i.redd.it/4h2sukb662e61.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:26:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37150</th>\n",
       "      <td>Could use some help here at the end of the day...</td>\n",
       "      <td>lt4i4u</td>\n",
       "      <td>https://i.redd.it/1rhnimof9vj61.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-27 04:37:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37151</th>\n",
       "      <td>The war has just begun</td>\n",
       "      <td>lt4fk0</td>\n",
       "      <td>https://v.redd.it/fcyia2xo8vj61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-27 04:34:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37152</th>\n",
       "      <td>PSA: AMCX is not up because of AMC</td>\n",
       "      <td>lt4ci5</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>Tl;dr AMCX is a very undervalued company based...</td>\n",
       "      <td>2021-02-27 04:30:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37153</th>\n",
       "      <td>Ape Combat 6: Liberation of Gamestopmeria</td>\n",
       "      <td>lt46qn</td>\n",
       "      <td>https://v.redd.it/x6b005am5vj61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-27 04:23:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37154</th>\n",
       "      <td>A Slap Back to Reality</td>\n",
       "      <td>lt47yg</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>Obligatory: This is not financial advice. I am...</td>\n",
       "      <td>2021-02-27 04:25:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37155 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title      id  \\\n",
       "0      It's not about the money, it's about sending a...  l6ulcx   \n",
       "1      Math Professor Scott Steiner says the numbers ...  l6uibd   \n",
       "2                                        Exit the system  l6uhhn   \n",
       "3      NEW SEC FILING FOR GME! CAN SOMEONE LESS RETAR...  l6ugk6   \n",
       "4      Not to distract from GME, just thought our AMC...  l6ufgy   \n",
       "...                                                  ...     ...   \n",
       "37150  Could use some help here at the end of the day...  lt4i4u   \n",
       "37151                             The war has just begun  lt4fk0   \n",
       "37152                 PSA: AMCX is not up because of AMC  lt4ci5   \n",
       "37153          Ape Combat 6: Liberation of Gamestopmeria  lt46qn   \n",
       "37154                             A Slap Back to Reality  lt47yg   \n",
       "\n",
       "                                                     url  \\\n",
       "0                        https://v.redd.it/6j75regs72e61   \n",
       "1                        https://v.redd.it/ah50lyny62e61   \n",
       "2      https://www.reddit.com/r/wallstreetbets/commen...   \n",
       "3      https://sec.report/Document/0001193125-21-019848/   \n",
       "4                    https://i.redd.it/4h2sukb662e61.jpg   \n",
       "...                                                  ...   \n",
       "37150                https://i.redd.it/1rhnimof9vj61.jpg   \n",
       "37151                    https://v.redd.it/fcyia2xo8vj61   \n",
       "37152  https://www.reddit.com/r/wallstreetbets/commen...   \n",
       "37153                    https://v.redd.it/x6b005am5vj61   \n",
       "37154  https://www.reddit.com/r/wallstreetbets/commen...   \n",
       "\n",
       "                                                    body            timestamp  \n",
       "0                                                    NaN  2021-01-28 21:37:41  \n",
       "1                                                    NaN  2021-01-28 21:32:10  \n",
       "2      The CEO of NASDAQ pushed to halt trading “to g...  2021-01-28 21:30:35  \n",
       "3                                                    NaN  2021-01-28 21:28:57  \n",
       "4                                                    NaN  2021-01-28 21:26:56  \n",
       "...                                                  ...                  ...  \n",
       "37150                                                NaN  2021-02-27 04:37:32  \n",
       "37151                                                NaN  2021-02-27 04:34:21  \n",
       "37152  Tl;dr AMCX is a very undervalued company based...  2021-02-27 04:30:42  \n",
       "37153                                                NaN  2021-02-27 04:23:42  \n",
       "37154  Obligatory: This is not financial advice. I am...  2021-02-27 04:25:09  \n",
       "\n",
       "[37155 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check data types ski_data.select_dtypes('object')\n",
    "wall.select_dtypes('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title            0\n",
       "score            0\n",
       "id               0\n",
       "url              0\n",
       "comms_num        0\n",
       "created          0\n",
       "body         18437\n",
       "timestamp        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at all posts with \"NaN\" see if you should remove them\n",
    "#ski_data[['AdultWeekend', 'AdultWeekday']].isnull().sum(axis=1)\n",
    "wall.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out how percent of missing \"body\" data\n",
    "# 37155/8437 = 2.01524109128. This is almost half the data\n",
    "\n",
    "\n",
    "missing = pd.concat([ski_data.isnull().sum(), 100 * ski_data.isnull().mean()], axis=1)\n",
    "missing.columns=['count', '%']\n",
    "missing.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19162</th>\n",
       "      <td>Times Square right now</td>\n",
       "      <td>348241</td>\n",
       "      <td>l8rf4k</td>\n",
       "      <td>https://v.redd.it/x64z70f7eie61</td>\n",
       "      <td>11554</td>\n",
       "      <td>1.612058e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-31 04:00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16009</th>\n",
       "      <td>GME YOLO update — Jan 28 2021</td>\n",
       "      <td>225870</td>\n",
       "      <td>l78uct</td>\n",
       "      <td>https://i.redd.it/opzucppb15e61.png</td>\n",
       "      <td>23309</td>\n",
       "      <td>1.611897e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-29 07:06:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17771</th>\n",
       "      <td>GME YOLO month-end update — Jan 2021</td>\n",
       "      <td>219779</td>\n",
       "      <td>l846a1</td>\n",
       "      <td>https://i.redd.it/r557em3t5ce61.png</td>\n",
       "      <td>20105</td>\n",
       "      <td>1.611983e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-30 07:04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34179</th>\n",
       "      <td>GME YOLO update — Feb 19 2021</td>\n",
       "      <td>201168</td>\n",
       "      <td>lnqgz8</td>\n",
       "      <td>https://i.redd.it/2xswz0h11ii61.png</td>\n",
       "      <td>12846</td>\n",
       "      <td>1.613798e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-20 07:05:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18273</th>\n",
       "      <td>It’s treason then</td>\n",
       "      <td>195782</td>\n",
       "      <td>l881ia</td>\n",
       "      <td>https://i.redd.it/d3t66lv1yce61.jpg</td>\n",
       "      <td>4523</td>\n",
       "      <td>1.611992e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-30 09:40:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23414</th>\n",
       "      <td>Greatest transfer of wealth, you say</td>\n",
       "      <td>0</td>\n",
       "      <td>lbk4m0</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>25</td>\n",
       "      <td>1.612374e+09</td>\n",
       "      <td>To the hedge funds maybe.\\n\\nYou think that ho...</td>\n",
       "      <td>2021-02-03 19:43:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35183</th>\n",
       "      <td>TC Energy (TRP.TO) making mad profits due to T...</td>\n",
       "      <td>0</td>\n",
       "      <td>lqzs0i</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>28</td>\n",
       "      <td>1.614160e+09</td>\n",
       "      <td>I will re-write this to r/WSB standards until ...</td>\n",
       "      <td>2021-02-24 11:42:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31107</th>\n",
       "      <td>MPRT! Amazon partnership and still going for low!</td>\n",
       "      <td>0</td>\n",
       "      <td>ldoall</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>11</td>\n",
       "      <td>1.612609e+09</td>\n",
       "      <td>Any thoughts on it with the Amazon announcement?</td>\n",
       "      <td>2021-02-06 12:56:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31108</th>\n",
       "      <td>CLOV is the New GME Change my mind! Hindenburg...</td>\n",
       "      <td>0</td>\n",
       "      <td>ldnk76</td>\n",
       "      <td>https://www.reddit.com/gallery/ldnk76</td>\n",
       "      <td>9</td>\n",
       "      <td>1.612606e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-06 12:14:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22092</th>\n",
       "      <td>How's the moon bois? XDDDDDDDDDDDDDDDD</td>\n",
       "      <td>0</td>\n",
       "      <td>laymap</td>\n",
       "      <td>https://i.redd.it/flay6hj153f61.jpg</td>\n",
       "      <td>26</td>\n",
       "      <td>1.612310e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-03 01:46:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37155 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title   score      id  \\\n",
       "19162                             Times Square right now  348241  l8rf4k   \n",
       "16009                      GME YOLO update — Jan 28 2021  225870  l78uct   \n",
       "17771               GME YOLO month-end update — Jan 2021  219779  l846a1   \n",
       "34179                      GME YOLO update — Feb 19 2021  201168  lnqgz8   \n",
       "18273                                  It’s treason then  195782  l881ia   \n",
       "...                                                  ...     ...     ...   \n",
       "23414               Greatest transfer of wealth, you say       0  lbk4m0   \n",
       "35183  TC Energy (TRP.TO) making mad profits due to T...       0  lqzs0i   \n",
       "31107  MPRT! Amazon partnership and still going for low!       0  ldoall   \n",
       "31108  CLOV is the New GME Change my mind! Hindenburg...       0  ldnk76   \n",
       "22092             How's the moon bois? XDDDDDDDDDDDDDDDD       0  laymap   \n",
       "\n",
       "                                                     url  comms_num  \\\n",
       "19162                    https://v.redd.it/x64z70f7eie61      11554   \n",
       "16009                https://i.redd.it/opzucppb15e61.png      23309   \n",
       "17771                https://i.redd.it/r557em3t5ce61.png      20105   \n",
       "34179                https://i.redd.it/2xswz0h11ii61.png      12846   \n",
       "18273                https://i.redd.it/d3t66lv1yce61.jpg       4523   \n",
       "...                                                  ...        ...   \n",
       "23414  https://www.reddit.com/r/wallstreetbets/commen...         25   \n",
       "35183  https://www.reddit.com/r/wallstreetbets/commen...         28   \n",
       "31107  https://www.reddit.com/r/wallstreetbets/commen...         11   \n",
       "31108              https://www.reddit.com/gallery/ldnk76          9   \n",
       "22092                https://i.redd.it/flay6hj153f61.jpg         26   \n",
       "\n",
       "            created                                               body  \\\n",
       "19162  1.612058e+09                                                NaN   \n",
       "16009  1.611897e+09                                                NaN   \n",
       "17771  1.611983e+09                                                NaN   \n",
       "34179  1.613798e+09                                                NaN   \n",
       "18273  1.611992e+09                                                NaN   \n",
       "...             ...                                                ...   \n",
       "23414  1.612374e+09  To the hedge funds maybe.\\n\\nYou think that ho...   \n",
       "35183  1.614160e+09  I will re-write this to r/WSB standards until ...   \n",
       "31107  1.612609e+09   Any thoughts on it with the Amazon announcement?   \n",
       "31108  1.612606e+09                                                NaN   \n",
       "22092  1.612310e+09                                                NaN   \n",
       "\n",
       "                 timestamp  \n",
       "19162  2021-01-31 04:00:38  \n",
       "16009  2021-01-29 07:06:23  \n",
       "17771  2021-01-30 07:04:45  \n",
       "34179  2021-02-20 07:05:55  \n",
       "18273  2021-01-30 09:40:59  \n",
       "...                    ...  \n",
       "23414  2021-02-03 19:43:51  \n",
       "35183  2021-02-24 11:42:18  \n",
       "31107  2021-02-06 12:56:10  \n",
       "31108  2021-02-06 12:14:50  \n",
       "22092  2021-02-03 01:46:32  \n",
       "\n",
       "[37155 rows x 8 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list 'score' column by desc\n",
    "#Filter the 'SkiableTerrain_ac' column to print the values greater than 10000\n",
    "#ski_data.loc[ski_data.SkiableTerrain_ac > 10000]\n",
    "\n",
    "wall.sort_values(by='score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time</th>\n",
       "      <th>Time (EST)</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's not about the money, it's about sending a...</td>\n",
       "      <td>55</td>\n",
       "      <td>l6ulcx</td>\n",
       "      <td>https://v.redd.it/6j75regs72e61</td>\n",
       "      <td>6</td>\n",
       "      <td>1.611863e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:37:41</td>\n",
       "      <td>21:37:41</td>\n",
       "      <td>21:37:41</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>21:37:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Math Professor Scott Steiner says the numbers ...</td>\n",
       "      <td>110</td>\n",
       "      <td>l6uibd</td>\n",
       "      <td>https://v.redd.it/ah50lyny62e61</td>\n",
       "      <td>23</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:32:10</td>\n",
       "      <td>21:32:10</td>\n",
       "      <td>21:32:10</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>21:32:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exit the system</td>\n",
       "      <td>0</td>\n",
       "      <td>l6uhhn</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>47</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>The CEO of NASDAQ pushed to halt trading “to g...</td>\n",
       "      <td>2021-01-28 21:30:35</td>\n",
       "      <td>21:30:35</td>\n",
       "      <td>21:30:35</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>21:30:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEW SEC FILING FOR GME! CAN SOMEONE LESS RETAR...</td>\n",
       "      <td>29</td>\n",
       "      <td>l6ugk6</td>\n",
       "      <td>https://sec.report/Document/0001193125-21-019848/</td>\n",
       "      <td>74</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:28:57</td>\n",
       "      <td>21:28:57</td>\n",
       "      <td>21:28:57</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>21:28:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not to distract from GME, just thought our AMC...</td>\n",
       "      <td>71</td>\n",
       "      <td>l6ufgy</td>\n",
       "      <td>https://i.redd.it/4h2sukb662e61.jpg</td>\n",
       "      <td>156</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:26:56</td>\n",
       "      <td>21:26:56</td>\n",
       "      <td>21:26:56</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>21:26:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score      id  \\\n",
       "0  It's not about the money, it's about sending a...     55  l6ulcx   \n",
       "1  Math Professor Scott Steiner says the numbers ...    110  l6uibd   \n",
       "2                                    Exit the system      0  l6uhhn   \n",
       "3  NEW SEC FILING FOR GME! CAN SOMEONE LESS RETAR...     29  l6ugk6   \n",
       "4  Not to distract from GME, just thought our AMC...     71  l6ufgy   \n",
       "\n",
       "                                                 url  comms_num       created  \\\n",
       "0                    https://v.redd.it/6j75regs72e61          6  1.611863e+09   \n",
       "1                    https://v.redd.it/ah50lyny62e61         23  1.611862e+09   \n",
       "2  https://www.reddit.com/r/wallstreetbets/commen...         47  1.611862e+09   \n",
       "3  https://sec.report/Document/0001193125-21-019848/         74  1.611862e+09   \n",
       "4                https://i.redd.it/4h2sukb662e61.jpg        156  1.611862e+09   \n",
       "\n",
       "                                                body            timestamp  \\\n",
       "0                                                NaN  2021-01-28 21:37:41   \n",
       "1                                                NaN  2021-01-28 21:32:10   \n",
       "2  The CEO of NASDAQ pushed to halt trading “to g...  2021-01-28 21:30:35   \n",
       "3                                                NaN  2021-01-28 21:28:57   \n",
       "4                                                NaN  2021-01-28 21:26:56   \n",
       "\n",
       "       time Time (EST)        Date      Time  \n",
       "0  21:37:41   21:37:41  2021-01-28  21:37:41  \n",
       "1  21:32:10   21:32:10  2021-01-28  21:32:10  \n",
       "2  21:30:35   21:30:35  2021-01-28  21:30:35  \n",
       "3  21:28:57   21:28:57  2021-01-28  21:28:57  \n",
       "4  21:26:56   21:26:56  2021-01-28  21:26:56  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract time from timestamp add an EST time column \n",
    "#resource- https://www.w3resource.com/python/python-date-and-time.php \n",
    "wall['Time'] = pd.DatetimeIndex(wall['timestamp']).time\n",
    "\n",
    "#change time format to EST \n",
    "wall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the mistake time columns\n",
    "\n",
    "wall.drop('time', inplace=True, axis=1)\n",
    "wall.drop('Time', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Time (EST)</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's not about the money, it's about sending a...</td>\n",
       "      <td>55</td>\n",
       "      <td>l6ulcx</td>\n",
       "      <td>https://v.redd.it/6j75regs72e61</td>\n",
       "      <td>6</td>\n",
       "      <td>1.611863e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:37:41</td>\n",
       "      <td>21:37:41</td>\n",
       "      <td>2021-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Math Professor Scott Steiner says the numbers ...</td>\n",
       "      <td>110</td>\n",
       "      <td>l6uibd</td>\n",
       "      <td>https://v.redd.it/ah50lyny62e61</td>\n",
       "      <td>23</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:32:10</td>\n",
       "      <td>21:32:10</td>\n",
       "      <td>2021-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exit the system</td>\n",
       "      <td>0</td>\n",
       "      <td>l6uhhn</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>47</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>The CEO of NASDAQ pushed to halt trading “to g...</td>\n",
       "      <td>2021-01-28 21:30:35</td>\n",
       "      <td>21:30:35</td>\n",
       "      <td>2021-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEW SEC FILING FOR GME! CAN SOMEONE LESS RETAR...</td>\n",
       "      <td>29</td>\n",
       "      <td>l6ugk6</td>\n",
       "      <td>https://sec.report/Document/0001193125-21-019848/</td>\n",
       "      <td>74</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:28:57</td>\n",
       "      <td>21:28:57</td>\n",
       "      <td>2021-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not to distract from GME, just thought our AMC...</td>\n",
       "      <td>71</td>\n",
       "      <td>l6ufgy</td>\n",
       "      <td>https://i.redd.it/4h2sukb662e61.jpg</td>\n",
       "      <td>156</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:26:56</td>\n",
       "      <td>21:26:56</td>\n",
       "      <td>2021-01-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score      id  \\\n",
       "0  It's not about the money, it's about sending a...     55  l6ulcx   \n",
       "1  Math Professor Scott Steiner says the numbers ...    110  l6uibd   \n",
       "2                                    Exit the system      0  l6uhhn   \n",
       "3  NEW SEC FILING FOR GME! CAN SOMEONE LESS RETAR...     29  l6ugk6   \n",
       "4  Not to distract from GME, just thought our AMC...     71  l6ufgy   \n",
       "\n",
       "                                                 url  comms_num       created  \\\n",
       "0                    https://v.redd.it/6j75regs72e61          6  1.611863e+09   \n",
       "1                    https://v.redd.it/ah50lyny62e61         23  1.611862e+09   \n",
       "2  https://www.reddit.com/r/wallstreetbets/commen...         47  1.611862e+09   \n",
       "3  https://sec.report/Document/0001193125-21-019848/         74  1.611862e+09   \n",
       "4                https://i.redd.it/4h2sukb662e61.jpg        156  1.611862e+09   \n",
       "\n",
       "                                                body            timestamp  \\\n",
       "0                                                NaN  2021-01-28 21:37:41   \n",
       "1                                                NaN  2021-01-28 21:32:10   \n",
       "2  The CEO of NASDAQ pushed to halt trading “to g...  2021-01-28 21:30:35   \n",
       "3                                                NaN  2021-01-28 21:28:57   \n",
       "4                                                NaN  2021-01-28 21:26:56   \n",
       "\n",
       "  Time (EST)        Date  \n",
       "0   21:37:41  2021-01-28  \n",
       "1   21:32:10  2021-01-28  \n",
       "2   21:30:35  2021-01-28  \n",
       "3   21:28:57  2021-01-28  \n",
       "4   21:26:56  2021-01-28  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract date from \"timestamp\" column and create \"Date\" column\n",
    "\n",
    "wall['Date'] = pd.DatetimeIndex(wall['timestamp']).date\n",
    "\n",
    "#change time format to EST \n",
    "wall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code task 18#\n",
    "#Call ski_data's `hist` method to plot histograms of each of the numeric features\n",
    "#Try passing it an argument figsize=(15,10)\n",
    "#Try calling plt.subplots_adjust() with an argument hspace=0.5 to adjust the spacing\n",
    "#It's important you create legible and easy-to-read plots\n",
    "ski_data.hist(figsize=(15,10))\n",
    "plt.subplots_adjust(hspace=0.5);\n",
    "#Hint: notice how the terminating ';' \"swallows\" some messy output and leads to a tidier notebook\n",
    "\n",
    "ski_data.SkiableTerrain_ac.hist(bins=30)\n",
    "plt.xlabel('SkiableTerrain_ac')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of skiable area (acres) after replacing erroneous value');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to check what data is missing\n",
    "\n",
    "''''You know there are two columns that refer to price: 'AdultWeekend' and 'AdultWeekday'. You can calculate the number of \n",
    "price values missing per row. This will obviously have to be either 0, 1, or 2, where 0 denotes no price values are missing \n",
    "and 2 denotes that both are missing.'''\n",
    "\n",
    "missing_price = ski_data[['AdultWeekend', 'AdultWeekday']].isnull().sum(axis=1)\n",
    "missing_price.value_counts()/len(missing_price) * 100\n",
    "\n",
    "#Code task 28#\n",
    "#Use `missing_price` to remove rows from ski_data where both price values are missing\n",
    "\n",
    "ski_data = ski_data[missing_price != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look out for extreme values in the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://builtin.com/data-science/step-step-explanation-principal-component-analysis\n",
    "#One way to disentangle this interconnected web of relationships is via principle components analysis (PCA). This technique will find linear combinations of the original features that are uncorrelated with one another and order them by the amount of variance they explain. You can use these derived features to visualize the data in a lower dimension (e.g. 2 down from 7) and know how much variance the representation explains. You can also explore how the original features contribute to these derived features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Frequency and Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AMC              39\n",
       "Robinhood        25\n",
       "HOLD THE LINE    20\n",
       "HOLD             16\n",
       "NOK              15\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#iterate over 'title' find consistent topics and aggregate them\n",
    "#Use pandas' Series method `value_counts` to find any duplicated resort names\n",
    "#ski_data['Name'].value_counts().head()\n",
    "#ski_data['Region'].value_counts()\n",
    "#use the `nunique` method to calculate, the number of unique values in each - !pip install pandas, ski_data[['Region', 'state']].nunique()\n",
    "\n",
    "wall['title'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph the most mentioned title \n",
    "\n",
    "wall['title'].value_counts().head().plot(kind='bar',color=['black', 'red', 'green', 'blue', 'cyan']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   title   score      id  \\\n",
      "19162                             Times Square right now  348241  l8rf4k   \n",
      "16009                      GME YOLO update — Jan 28 2021  225870  l78uct   \n",
      "17771               GME YOLO month-end update — Jan 2021  219779  l846a1   \n",
      "34179                      GME YOLO update — Feb 19 2021  201168  lnqgz8   \n",
      "18273                                  It’s treason then  195782  l881ia   \n",
      "18069  Used some of my GME tendies to buy Nintendo Sw...  192980  l8c0u4   \n",
      "32168                   Wow super bowl commercial for us  191380  lexy8t   \n",
      "18230                              That’s what I thought  175665  l890i7   \n",
      "16220                      IT'S POWER TO THE TRADERS NOW  174401  l7feld   \n",
      "18826  I am proud to do my part in paying forward our...  171778  l90oq6   \n",
      "\n",
      "                                                     url  comms_num  \\\n",
      "19162                    https://v.redd.it/x64z70f7eie61      11554   \n",
      "16009                https://i.redd.it/opzucppb15e61.png      23309   \n",
      "17771                https://i.redd.it/r557em3t5ce61.png      20105   \n",
      "34179                https://i.redd.it/2xswz0h11ii61.png      12846   \n",
      "18273                https://i.redd.it/d3t66lv1yce61.jpg       4523   \n",
      "18069  https://www.nbcdfw.com/news/local/north-texas-...       6785   \n",
      "32168                https://i.redd.it/tko5nh1qy4g61.jpg       3849   \n",
      "18230                https://i.redd.it/f2yhugwt5de61.jpg       4603   \n",
      "16220                    https://v.redd.it/lu8aekujd6e61       5211   \n",
      "18826                https://i.redd.it/sgrbt9vthke61.jpg       3633   \n",
      "\n",
      "            created body            timestamp Time (EST)        Date  \n",
      "19162  1.612058e+09  NaN  2021-01-31 04:00:38   04:00:38  2021-01-31  \n",
      "16009  1.611897e+09  NaN  2021-01-29 07:06:23   07:06:23  2021-01-29  \n",
      "17771  1.611983e+09  NaN  2021-01-30 07:04:45   07:04:45  2021-01-30  \n",
      "34179  1.613798e+09  NaN  2021-02-20 07:05:55   07:05:55  2021-02-20  \n",
      "18273  1.611992e+09  NaN  2021-01-30 09:40:59   09:40:59  2021-01-30  \n",
      "18069  1.612004e+09  NaN  2021-01-30 12:53:40   12:53:40  2021-01-30  \n",
      "32168  1.612768e+09  NaN  2021-02-08 08:58:50   08:58:50  2021-02-08  \n",
      "18230  1.611995e+09  NaN  2021-01-30 10:24:35   10:24:35  2021-01-30  \n",
      "16220  1.611913e+09  NaN  2021-01-29 11:35:58   11:35:58  2021-01-29  \n",
      "18826  1.612084e+09  NaN  2021-01-31 11:04:23   11:04:23  2021-01-31  \n"
     ]
    }
   ],
   "source": [
    "#see which topics scored the highest, top 10 scores\n",
    "#(ski_data.Region!= ski_data.state).value_counts(),  ski_data['Region'].value_counts()\n",
    "\n",
    "top_10_score = wall.nlargest(10,['score'])\n",
    "\n",
    "print(top_10_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-fc86204547ad>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-fc86204547ad>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    print(top_10_score_body)\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#find the top 10 scored posts that have a \"body\"\n",
    "\n",
    "ski_data[ski_data['Name'] == 'Crystal Mountain']\n",
    "\n",
    "#top_10_score_body = wall.nlargest(10,['score', 'body'!= 'NaN'])\n",
    "#tom_and_42 = df[(df[\"Name\"]==\"Tom\") & (df[\"Age\"]==42)]\n",
    "\n",
    "top_10_score_body = wall.nlargest(10,['score'] & wall.nlargest(10,['body'!= 'NaN'])\n",
    "\n",
    "print(top_10_score_body)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the top 10 posts with bodies of texts \n",
    "\n",
    "\n",
    "wall['title'].value_counts().head().plot(kind='bar',color=['black', 'red', 'green', 'blue', 'cyan']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over 'body' search for specific words and aggregate them\n",
    "\n",
    "#We will use wordclouds to identify the most frequent words in the titles and body of the posts.\n",
    "#code from wallstreet project on kaggle\n",
    "#Title \n",
    "\n",
    "def show_wordcloud(data, title=\"\"):\n",
    "    text = \" \".join(t for t in data.dropna())\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update([\"t\", \"co\", \"https\", \"amp\", \"U\", \"fuck\", \"fucking\"])\n",
    "    wordcloud = WordCloud(stopwords=stopwords, scale=4, max_font_size=50, max_words=500,background_color=\"black\").generate(text)\n",
    "    fig = plt.figure(1, figsize=(16,16))\n",
    "    plt.axis('off')\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    fig.subplots_adjust(top=2.3)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.show()\n",
    "    \n",
    "    show_wordcloud(wall['title'], title = 'Prevalent words in titles')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wall' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-04ff2e607b63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Body\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mshow_wordcloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwall\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Prevalent words in post bodies'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'wall' is not defined"
     ]
    }
   ],
   "source": [
    "#code from wallstreet project on kaggle\n",
    "#Body\n",
    "\n",
    "show_wordcloud(wall['body'], title = 'Prevalent words in post bodies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What seems to make a post ranked higher than others? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate number of posts in an average month \n",
    "\n",
    "mean_headlines = round(np.mean(headline_counts))\n",
    "print('Mean number of headlines in each month of Kaggle data:', mean_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timestamp Parsing and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what days of the week got the most posts?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What day of the week got most rated posts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatterplot/Bokeh the correlation between days of the week and most posts and highest rated posts.\n",
    "#scatterplot score v time v title \n",
    "#resource https://jakevdp.github.io/PythonDataScienceHandbook/04.02-simple-scatter-plots.html\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import numpy as np\n",
    "\n",
    "#Code task 36#\n",
    "#Use ski_data's `plot()` method to create a scatterplot (kind='scatter') with 'AdultWeekday' on the x-axis and\n",
    "#'AdultWeekend' on the y-axis\n",
    "\n",
    "ski_data.plot(x='AdultWeekday', y='AdultWeekend', kind= 'scatter');\n",
    "\n",
    "x = np.linspace(0, 10, 30)\n",
    "y = np.sin(x)\n",
    "\n",
    "plt.plot('score', 'date', 'o', color='black');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what time during the day got the most posts?\n",
    "\n",
    "top_posts = wall.nlargest(10,['score'] & wall.(10,['body'!= 'NaN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what time during the day were posts highly rated?\n",
    "#score v time\n",
    "\n",
    "top_posts = wall.nlargest(10,['score'] & wall.(10,['body'!= 'NaN'])\n",
    "                                  \n",
    "print(top_posts)  \n",
    " \n",
    "\n",
    "column_1 = wall[\"score\"]\n",
    "column_2 = wall[\"Date\"]\n",
    "correlation = column_1.corr(column_2)                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatterplot/Bokeh the correlation between time of the day and most posts and highest rated posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is there a correlation between the words that showed up the most in the (body) and the time of day? (tope 10 words)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is there a correlation between the words that showed up the most in the (body) and the day of the week? (tope 10 words)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatterplot/Bokeh the correlation between most used words in the (body) and the time and day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vizualizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title of posts\n",
    "\n",
    "def show_wordcloud(data, title=\"\"):\n",
    "    text = \" \".join(t for t in data.dropna())\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update([\"t\", \"co\", \"https\", \"amp\", \"U\", \"fuck\", \"fucking\"])\n",
    "    wordcloud = WordCloud(stopwords=stopwords, scale=4, max_font_size=50, max_words=500,background_color=\"black\").generate(text)\n",
    "    fig = plt.figure(1, figsize=(16,16))\n",
    "    plt.axis('off')\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    fig.subplots_adjust(top=2.3)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STOPWORDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ff44242fe6dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#body\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mshow_wordcloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwall\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Prevalent words in post bodies'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-a18776f8ffca>\u001b[0m in \u001b[0;36mshow_wordcloud\u001b[1;34m(data, title)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mshow_wordcloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"co\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"https\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"amp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"U\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fuck\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fucking\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mwordcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_font_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"black\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'STOPWORDS' is not defined"
     ]
    }
   ],
   "source": [
    "#body\n",
    "\n",
    "show_wordcloud(wall['body'], title = 'Prevalent words in post bodies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment analysis body of texts\n",
    "#change to body\n",
    "\n",
    "# borrowed from https://www.kaggle.com/pashupatigupta/sentiments-transformer-vader-embedding-bert\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "def find_sentiment(post):\n",
    "    if sia.polarity_scores(post)[\"compound\"] > 0:\n",
    "        return \"Positive\"\n",
    "    elif sia.polarity_scores(post)[\"compound\"] < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"       \n",
    "    \n",
    "def plot_sentiment(df, feature, title):\n",
    "    counts = df[feature].value_counts()\n",
    "    percent = counts/sum(counts)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "    counts.plot(kind='bar', ax=ax1, color='green')\n",
    "    percent.plot(kind='bar', ax=ax2, color='blue')\n",
    "    ax1.set_ylabel(f'Counts : {title} sentiments', size=12)\n",
    "    ax2.set_ylabel(f'Percentage : {title} sentiments', size=12)\n",
    "    plt.suptitle(f\"Sentiment analysis: {title}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()    def plot_sentiment(df, feature, title):\n",
    "    counts = df[feature].value_counts()\n",
    "    percent = counts/sum(counts)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "    counts.plot(kind='bar', ax=ax1, color='green')\n",
    "    percent.plot(kind='bar', ax=ax2, color='blue')\n",
    "    ax1.set_ylabel(f'Counts : {title} sentiments', size=12)\n",
    "    ax2.set_ylabel(f'Percentage : {title} sentiments', size=12)\n",
    "    plt.suptitle(f\"Sentiment analysis: {title}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "df = data_df.loc[~data_df.body.isna()]\n",
    "df['body_sentiment'] = df['body'].apply(lambda x: find_sentiment(x))\n",
    "plot_sentiment(df, 'body_sentiment', 'Body')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Analyzation \n",
    "\n",
    "1. stemming\n",
    "2. lemmatization\n",
    "3. tokenization\n",
    "4. stop word removal\n",
    "4. frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "#https://www.geeksforgeeks.org/python-stemming-words-with-nltk/\n",
    "\n",
    "# import these modules\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "   \n",
    "ps = PorterStemmer()\n",
    "  \n",
    "# choose some words to be stemmed\n",
    "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"]\n",
    "  \n",
    "for w in words:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization \n",
    "\n",
    "corpus = all_headlines_uniq\n",
    "\n",
    "# Clean and tokenize using Keras' built-in Tokenizer() method\n",
    "tokenizer = Tokenizer() # create the tokenizer\n",
    "tokenizer.fit_on_texts(corpus) # fit the tokenizer on the documents\n",
    "\n",
    "# convert data to sequence of tokens\n",
    "input_sequences = [] # sequences of n-grams from all documents in the corpus\n",
    "for headline in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([headline])[0] # list of tokens corresponding to each word in the document (i.e., news headline)\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1] # creates n-gram sequence (from 2 word n-grams, through n-grams = len(headline))\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "print(\"Printing the first 5 input sequences...\")\n",
    "print(input_sequences[:10])\n",
    "\n",
    "nb_samples = sum(len(s) for s in input_sequences) # total number of samples in input_sequences; \n",
    "print(\"\\nTotal number of samples in input_sequences:\", nb_samples)\n",
    "\n",
    "# Vocab size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"\\nTotal number of words in the vocabulary:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding\n",
    "\n",
    "max_sequence_len = max([len(s) for s in input_sequences]) # find length of the longest input sequence (i.e., headline)\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')) # pads the input sequences and converts to a numpy array (must fit model on np arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop word removal \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "import io \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "# word_tokenize accepts\n",
    "# a string as an input, not a file. \n",
    "stop_words = set(stopwords.words('english')) \n",
    "file1 = open(\"text.txt\") \n",
    "  \n",
    "# Use this to read file content as a stream: \n",
    "line = file1.read()\n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('filteredtext.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Did posts correlate to real stock picking and market movements \n",
    "2. Can we track stock picks and retail buyers strategies based on reddit posts?\n",
    "3. build ongoing scrapping attached to sentiment report- Quiver Quant’s Wall Street Bets Sentiment Tracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using API data\n",
    "#https://seekingalpha.com/article/4407134-future-wallstreetbets-stocks-characteristics-of-meme-stocks\n",
    "#https://bullish.news/wall-street-bets-next-big-trade/\n",
    "\n",
    "\n",
    "def get_NYT_headlines(year, month):\n",
    "    \n",
    "    '''\n",
    "     This function makes a request to the New York Times Archive API and collects \n",
    "     a list of article headlines for the specified month and year of interest.\n",
    "    '''\n",
    "\n",
    "    api_key = {'api-key' : pickle.load(open('apikey.pkl','rb'))}\n",
    "    url = 'https://api.nytimes.com/svc/archive/v1/' + str(year) + '/' + str(month) + '.json'\n",
    "    \n",
    "    response = requests.get(url, params=api_key)\n",
    "    output = response.json()\n",
    "    \n",
    "    docs = output['response']['docs']\n",
    "    \n",
    "    headlines = []\n",
    "    for doc in docs:\n",
    "        headlines.append(doc['headline']['main'])\n",
    "    \n",
    "    return headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#web scraping code \n",
    "#https://towardsdatascience.com/stock-market-analysis-in-python-part-1-getting-data-by-web-scraping-cb0589aca178\n",
    "#https://towardsdatascience.com/python-how-to-get-live-market-data-less-than-0-1-second-lag-c85ee280ed93\n",
    "\n",
    "import requests # for http requests\n",
    "from bs4 import BeautifulSoup # for html parsing and scraping\n",
    "import bs4\n",
    "from fastnumbers import isfloat \n",
    "from fastnumbers import fast_float\n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from tidylib import tidy_document # for tidying incorrect html\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#String to float conversion\n",
    "\n",
    "def ffloat(string):\n",
    "    if string is None:\n",
    "        return np.nan\n",
    "    if type(string)==float or type(string)==np.float64:\n",
    "        return string\n",
    "    if type(string)==int or type(string)==np.int64:\n",
    "        return string\n",
    "    return fast_float(string.split(\" \")[0].replace(',','').replace('%',''),\n",
    "                      default=np.nan)\n",
    "#or \n",
    "\n",
    "def ffloat_list(string_list):\n",
    "    return list(map(ffloat,string_list))\n",
    "\n",
    "#Removing Multple spaces from within string\n",
    "def remove_multiple_spaces(string):\n",
    "    if type(string)==str:\n",
    "        return ' '.join(string.split())\n",
    "    return string\n",
    "\n",
    "#Making Http Requests\n",
    "response = requests.get(\"http://www.example.com/\", timeout=240)\n",
    "response.status_code\n",
    "response.content\n",
    "\n",
    "#Getting Json content and parsing it\n",
    "url = \"https://jsonplaceholder.typicode.com/posts/1\"\n",
    "response = requests.get(url, timeout=240)\n",
    "response.status_code\n",
    "response.json()\n",
    "\n",
    "content = page_response.json()\n",
    "content.keys()\n",
    "\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge web scraping and API data \n",
    "\n",
    "all_headlines = kaggle_headlines + may_2018_rand\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modeling posts data \n",
    "\n",
    "## create predictors and label\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1] # label is the last word of each n-gram, and predictors are all the preceeding words\n",
    "label = ku.to_categorical(label, num_classes=vocab_size) # one-hot encodes the labels; matrix dimension are the number of input sequences by the number of total words/tokens in the word dictionary\n",
    "\n",
    "\n",
    "#use model to predict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Questions\n",
    "\n",
    "In your quest, you might need to ask a bunch of other questions, such as:\n",
    "\n",
    "1. Can I count something interesting?\n",
    "\n",
    "-I'm trying to find most popular words(topics) seen in the title and in the bodies of the Reddit posts. I'm also trying to find the correlation between what time and day that useful comments are posted. Can someone who's trying to get ahead of trends first in reddit to get ahead of trends in the markets, pinpoint the signal in the noise (who, when, and what). \n",
    "\n",
    "\n",
    "2. Can I find trends (e.g. high, low, increasing, decreasing, anomalies)?\n",
    "\n",
    "\n",
    "3. Can I make a bar plot or a histogram?\n",
    "\n",
    "-Yes, correlation between popularity of topics and time of post\n",
    "\n",
    "- Can I compare two related quantities?\n",
    "\n",
    "- Can I make a scatterplot?\n",
    "\n",
    "- Can I make a time-series plot?\n",
    "\n",
    "- Looking at the plots, what are some insights I can make?\n",
    "\n",
    "- Can I see any correlations?\n",
    "\n",
    "- Is there a hypothesis I can - and should - investigate further?\n",
    "\n",
    "- What other questions are the insights leading me to ask?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot frequency of topics brought up against the date and time \n",
    "#resources: https://datavizcatalogue.com/search/time.html\n",
    "\n",
    "#subplots frequency of topics in topics against time and date- USE type\n",
    "\n",
    "#plot topic versus scores see if theres any consistency there\n",
    "\n",
    "#options\n",
    "\n",
    "#Code task 13#\n",
    "#Create two subplots on 1 row and 2 columns with a figsize of (12, 8)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n",
    "#Specify a horizontal barplot ('barh') as kind of plot (kind=)\n",
    "ski_data.Region.value_counts().plot(kind='barh', ax=ax[0])\n",
    "#Give the plot a helpful title of 'Region'\n",
    "ax[0].set_title('Region')\n",
    "#Label the xaxis 'Count'\n",
    "ax[0].set_xlabel('Count')\n",
    "\n",
    "#Specify a horizontal barplot ('barh') as kind of plot (kind=)\n",
    "ski_data.state.value_counts().plot(kind='barh', ax=ax[1])\n",
    "#Give the plot a helpful title of 'state'\n",
    "ax[1].set_title('state')\n",
    "#Label the xaxis 'Count'\n",
    "ax[1].set_xlabel('Count')\n",
    "#Give the subplots a little \"breathing room\" with a wspace of 0.5\n",
    "plt.subplots_adjust(wspace=0.5);\n",
    "\n",
    "#You're encouraged to explore a few different figure sizes, orientations, and spacing here\n",
    "# as the importance of easy-to-read and informative figures is frequently understated\n",
    "# and you will find the ability to tweak figures invaluable later on\n",
    "\n",
    "# The next bit simply reorders the index by increasing average of weekday and weekend prices\n",
    "# Compare the index order you get from\n",
    "# state_price_means.index\n",
    "# with\n",
    "# state_price_means.mean(axis=1).sort_values(ascending=False).index\n",
    "# See how this expression simply sits within the reindex()\n",
    "(state_price_means.reindex(index=state_price_means.mean(axis=1)\n",
    "    .sort_values(ascending=False)\n",
    "    .index)\n",
    "    .plot(kind='barh', figsize=(10, 10), title='Average ticket price by State'))\n",
    "plt.xlabel('Price ($)');\n",
    "\n",
    "\n",
    "#Code task 18#\n",
    "#Call ski_data's `hist` method to plot histograms of each of the numeric features\n",
    "#Try passing it an argument figsize=(15,10)\n",
    "#Try calling plt.subplots_adjust() with an argument hspace=0.5 to adjust the spacing\n",
    "#It's important you create legible and easy-to-read plots\n",
    "ski_data.hist(figsize=(15,10))\n",
    "plt.subplots_adjust(hspace=0.5);\n",
    "#Hint: notice how the terminating ';' \"swallows\" some messy output and leads to a tidier notebook\n",
    "\n",
    "ski_data.SkiableTerrain_ac.hist(bins=30)\n",
    "plt.xlabel('SkiableTerrain_ac')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of skiable area (acres) after replacing erroneous value');\n",
    "\n",
    "#Use ski_data's `plot()` method to create a scatterplot (kind='scatter') with 'AdultWeekday' on the x-axis and\n",
    "#'AdultWeekend' on the y-axis\n",
    "\n",
    "ski_data.plot(x='AdultWeekday', y='AdultWeekend', kind= 'scatter');\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Specify a horizontal barplot ('barh') as kind of plot (kind=)\n",
    "ski_data.state.value_counts().plot(kind='barh', ax=ax[1])\n",
    "#Give the plot a helpful title of 'state'\n",
    "ax[1].set_title('state')\n",
    "#Label the xaxis 'Count'\n",
    "ax[1].set_xlabel('Count')\n",
    "#Give the subplots a little \"breathing room\" with a wspace of 0.5\n",
    "plt.subplots_adjust(wspace=0.5);\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n",
    "#Specify a horizontal barplot ('barh') as kind of plot (kind=)\n",
    "wall.score.value_counts().plot(kind='barh', ax=ax[0])\n",
    "#Give the plot a helpful title of 'Region'\n",
    "ax[0].set_title('score')\n",
    "#Label the xaxis 'Count'\n",
    "ax[0].set_xlabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#which words in the body showed up the most\n",
    "\n",
    "#Sentence Tokenization\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)\n",
    "\n",
    "#frequency distribution \n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(tokenized_word)\n",
    "print(fdist)\n",
    "\n",
    "<FreqDist with 25 samples and 30 outcomes>\n",
    "\n",
    "#datetime changing code \n",
    "\n",
    "current_day = datetime.date.today()\n",
    "print(\"\\n Default Date Object:\", current_day, \"\\n\")\n",
    " \n",
    "formatted_date = datetime.date.strftime(current_day, \"%m/%d/%Y\")\n",
    "print(\"\\n Formatted Date String:\", formatted_date, \"\\n\")\n",
    "\n",
    "\n",
    "#split timestamp into a (MM/DD/YY) format, and a column for time in EST\n",
    "#Use the .loc accessor again to modify this value with the correct value of 1819\n",
    "ski_data.loc[39, 'SkiableTerrain_ac'] = 1819\n",
    "wall.loc\n",
    "\n",
    "#Use pandas' Series' `replace()` method to replace anything within square brackets (including the brackets) - usa_states_sub.state.replace(to_replace='\\[.*\\]', value='', regex=True, inplace=True)\n",
    "usa_states_sub.state[usa_states_sub.state.str.contains('Massachusetts|Pennsylvania|Rhode Island|Virginia')]\n",
    "\n",
    "\n",
    "#transpose method of most talked about topics, but you can access this conveniently with the `T`: property.ski_data[ski_data.Name == 'Big Mountain Resort'].T\n",
    "\n",
    "##Call ski_data's `describe` method for a statistical summary of the numerical columns\n",
    "#Hint: there are fewer summary stat columns than features, so displaying the transpose\n",
    "#will be useful again\n",
    "\n",
    "ski_data.describe().T\n",
    "\n",
    "merg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"It's not about the money, it's about sending a message. 🚀💎🙌\",\n",
       "       'Math Professor Scott Steiner says the numbers spell DISASTER for Gamestop shorts',\n",
       "       'Exit the system', ..., 'PSA: AMCX is not up because of AMC',\n",
       "       'Ape Combat 6: Liberation of Gamestopmeria',\n",
       "       'A Slap Back to Reality'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wall['title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wall.['title']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5437</th>\n",
       "      <td>🪰 ME TO THE 🌝 $AAL 🛫📈</td>\n",
       "      <td>0</td>\n",
       "      <td>l6zjuc</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.611876e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-29 01:23:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7031</th>\n",
       "      <td>🩸🩸🩸THIS IS WHEN WE MUST HOLD AND STAND TALL......</td>\n",
       "      <td>1</td>\n",
       "      <td>l708dz</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.611878e+09</td>\n",
       "      <td>If everyone likes a football team they go BUY ...</td>\n",
       "      <td>2021-01-29 01:47:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8813</th>\n",
       "      <td>🩸 🩸 🩸 RATE THEM DOWN ON GOOGLE PLAY / APP STORE</td>\n",
       "      <td>1</td>\n",
       "      <td>l70xg4</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.611879e+09</td>\n",
       "      <td>Upvote so more people see it. Robinhood and th...</td>\n",
       "      <td>2021-01-29 02:13:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8491</th>\n",
       "      <td>🩸 🩸 🩸 RATE THEM DOWN ON GOOGLE PLAY / APP STORE</td>\n",
       "      <td>1</td>\n",
       "      <td>l70l1c</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.611878e+09</td>\n",
       "      <td>Upvote so more people see it. Robinhood and th...</td>\n",
       "      <td>2021-01-29 02:00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10208</th>\n",
       "      <td>🩸 🩸 🩸 RATE THEM DOWN ON GOOGLE PLAY / APP STORE</td>\n",
       "      <td>1</td>\n",
       "      <td>l71evw</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.611880e+09</td>\n",
       "      <td>Upvote so more people see it. Robinhood and th...</td>\n",
       "      <td>2021-01-29 02:29:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20834</th>\n",
       "      <td>!!!! HOLD HOLD HOLD !!!! Buy the dip !!</td>\n",
       "      <td>160</td>\n",
       "      <td>laonm1</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>34</td>\n",
       "      <td>1.612273e+09</td>\n",
       "      <td>I am new here but I had AMC and GME when the p...</td>\n",
       "      <td>2021-02-02 15:41:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10706</th>\n",
       "      <td>!!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>l71a9q</td>\n",
       "      <td>https://i.redd.it/sb05b2pan3e61.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1.611880e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-29 02:25:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4854</th>\n",
       "      <td>!!! CHAMATH, Please open your own trading plat...</td>\n",
       "      <td>4</td>\n",
       "      <td>l6ziga</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.611876e+09</td>\n",
       "      <td>Wall street bets is getting shutdown.\\n\\nLess...</td>\n",
       "      <td>2021-01-29 01:22:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3794</th>\n",
       "      <td>!!! CHAMATH, Please open your own trading plat...</td>\n",
       "      <td>6</td>\n",
       "      <td>l6zdyh</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.611876e+09</td>\n",
       "      <td>Wall street bets is getting shutdown.\\n\\nLess...</td>\n",
       "      <td>2021-01-29 01:17:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>! STOP SELLING BB !</td>\n",
       "      <td>11</td>\n",
       "      <td>l6yvm2</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.611875e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-29 01:00:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37155 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  score      id  \\\n",
       "5437                               🪰 ME TO THE 🌝 $AAL 🛫📈      0  l6zjuc   \n",
       "7031   🩸🩸🩸THIS IS WHEN WE MUST HOLD AND STAND TALL......      1  l708dz   \n",
       "8813     🩸 🩸 🩸 RATE THEM DOWN ON GOOGLE PLAY / APP STORE      1  l70xg4   \n",
       "8491     🩸 🩸 🩸 RATE THEM DOWN ON GOOGLE PLAY / APP STORE      1  l70l1c   \n",
       "10208    🩸 🩸 🩸 RATE THEM DOWN ON GOOGLE PLAY / APP STORE      1  l71evw   \n",
       "...                                                  ...    ...     ...   \n",
       "20834            !!!! HOLD HOLD HOLD !!!! Buy the dip !!    160  laonm1   \n",
       "10706                                               !!!!      1  l71a9q   \n",
       "4854   !!! CHAMATH, Please open your own trading plat...      4  l6ziga   \n",
       "3794   !!! CHAMATH, Please open your own trading plat...      6  l6zdyh   \n",
       "1698                                 ! STOP SELLING BB !     11  l6yvm2   \n",
       "\n",
       "                                                     url  comms_num  \\\n",
       "5437   https://www.reddit.com/r/wallstreetbets/commen...          0   \n",
       "7031   https://www.reddit.com/r/wallstreetbets/commen...          0   \n",
       "8813   https://www.reddit.com/r/wallstreetbets/commen...          0   \n",
       "8491   https://www.reddit.com/r/wallstreetbets/commen...          0   \n",
       "10208  https://www.reddit.com/r/wallstreetbets/commen...          0   \n",
       "...                                                  ...        ...   \n",
       "20834  https://www.reddit.com/r/wallstreetbets/commen...         34   \n",
       "10706                https://i.redd.it/sb05b2pan3e61.png          0   \n",
       "4854   https://www.reddit.com/r/wallstreetbets/commen...          1   \n",
       "3794   https://www.reddit.com/r/wallstreetbets/commen...          0   \n",
       "1698   https://www.reddit.com/r/wallstreetbets/commen...          1   \n",
       "\n",
       "            created                                               body  \\\n",
       "5437   1.611876e+09                                                NaN   \n",
       "7031   1.611878e+09  If everyone likes a football team they go BUY ...   \n",
       "8813   1.611879e+09  Upvote so more people see it. Robinhood and th...   \n",
       "8491   1.611878e+09  Upvote so more people see it. Robinhood and th...   \n",
       "10208  1.611880e+09  Upvote so more people see it. Robinhood and th...   \n",
       "...             ...                                                ...   \n",
       "20834  1.612273e+09  I am new here but I had AMC and GME when the p...   \n",
       "10706  1.611880e+09                                                NaN   \n",
       "4854   1.611876e+09   Wall street bets is getting shutdown.\\n\\nLess...   \n",
       "3794   1.611876e+09   Wall street bets is getting shutdown.\\n\\nLess...   \n",
       "1698   1.611875e+09                                                NaN   \n",
       "\n",
       "                 timestamp  \n",
       "5437   2021-01-29 01:23:38  \n",
       "7031   2021-01-29 01:47:19  \n",
       "8813   2021-01-29 02:13:13  \n",
       "8491   2021-01-29 02:00:29  \n",
       "10208  2021-01-29 02:29:48  \n",
       "...                    ...  \n",
       "20834  2021-02-02 15:41:58  \n",
       "10706  2021-01-29 02:25:34  \n",
       "4854   2021-01-29 01:22:18  \n",
       "3794   2021-01-29 01:17:58  \n",
       "1698   2021-01-29 01:00:57  \n",
       "\n",
       "[37155 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wall.sort_values(by='title', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'findall'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-6365ddbba8f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#wall['title'] = wall.text.str.findall('chamath')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mwall\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'chamath'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5139\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5140\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5141\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'findall'"
     ]
    }
   ],
   "source": [
    "#aggregate certain topics together\n",
    "\n",
    "#wall['title'].find('chamath')\n",
    "\n",
    "#wall['title'] = wall.text.str.findall('chamath')\n",
    "\n",
    "wall.findall('chamath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort all titles and aggregate the topics\n",
    "#show how often certain topics are brought up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#than show how with these comments, at what point did they become more frequent, use dates, \n",
    "# what times were most of these comments made\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wall['body'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index\n",
    "properties_T = properties_T.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-141dc0e39ea9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#assign the values of the first row to your column headings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mwall_T\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "np.transpose\n",
    "\n",
    "wall_T = wall.T\n",
    "\n",
    "#assign the values of the first row to your column headings\n",
    "\n",
    "df.columns = df.iloc[1]\n",
    "wall_T.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\Allegra Grunberg/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\dev\\\\nltk_data'\n    - 'C:\\\\dev\\\\share\\\\nltk_data'\n    - 'C:\\\\dev\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Allegra Grunberg\\\\AppData\\\\Roaming\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-4a9dd92165dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msia\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_sentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"compound\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lexicon_file)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \"\"\"\n\u001b[0;32m    199\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlexicon_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexicon_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlexicon_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_lex_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 836\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\Allegra Grunberg/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\dev\\\\nltk_data'\n    - 'C:\\\\dev\\\\share\\\\nltk_data'\n    - 'C:\\\\dev\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Allegra Grunberg\\\\AppData\\\\Roaming\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#Sentiment analysis, With nltk SentimentIntensityAnalyzer\n",
    "# borrowed from https://www.kaggle.com/pashupatigupta/sentiments-transformer-vader-embedding-bert\n",
    "#Title\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "def find_sentiment(post):\n",
    "    if sia.polarity_scores(post)[\"compound\"] > 0:\n",
    "        return \"Positive\"\n",
    "    elif sia.polarity_scores(post)[\"compound\"] < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"   \n",
    "    \n",
    "def plot_sentiment(df, feature, title):\n",
    "    counts = df[feature].value_counts()\n",
    "    percent = counts/sum(counts)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "    counts.plot(kind='bar', ax=ax1, color='green')\n",
    "    percent.plot(kind='bar', ax=ax2, color='blue')\n",
    "    ax1.set_ylabel(f'Counts : {title} sentiments', size=12)\n",
    "    ax2.set_ylabel(f'Percentage : {title} sentiments', size=12)\n",
    "    plt.suptitle(f\"Sentiment analysis: {title}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "data_df['title_sentiment'] = data_df['title'].apply(lambda x: find_sentiment(x))\n",
    "plot_sentiment(data_df, 'title_sentiment', 'Title')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
